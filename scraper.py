import asyncio
import aiohttp
import json
import re
import logging
from bs4 import BeautifulSoup
import os
import shutil
from datetime import datetime
import pytz
import base64
from urllib.parse import parse_qs, unquote

# --- é…ç½®å¸¸é‡ ---
CONFIG_DIR = 'config'  # é…ç½®æ–‡ä»¶å¤¹ï¼Œç”¨äºå­˜æ”¾è¾“å…¥æ–‡ä»¶
URLS_FILE = os.path.join(CONFIG_DIR, 'urls.txt')
KEYWORDS_FILE = os.path.join(CONFIG_DIR, 'keywords.json') # åº”åŒ…å«å›½å®¶çš„ä¸¤å­—æ¯ä»£ç 
OUTPUT_DIR = 'output_configs'
COUNTRY_SUBDIR = 'countries'  # å›½å®¶é…ç½®æ–‡ä»¶å¤¹
PROTOCOL_SUBDIR = 'protocols' # åè®®é…ç½®æ–‡ä»¶å¤¹
README_FILE = 'README.md'
REQUEST_TIMEOUT = 15
CONCURRENT_REQUESTS = 10
MAX_CONFIG_LENGTH = 1500
MIN_PERCENT25_COUNT = 15
FILTERED_PHRASE = 'i_love_'  # è¦è¿‡æ»¤çš„ç‰¹å®šçŸ­è¯­

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# --- åè®®ç±»åˆ« ---
PROTOCOL_CATEGORIES = [
    "Vmess", "Vless", "Trojan", "ShadowSocks", "ShadowSocksR",
    "Tuic", "Hysteria2", "WireGuard"
]
# é¢„ç¼–è¯‘åè®®å‰ç¼€åˆ—è¡¨ï¼Œæé«˜æ€§èƒ½
PROTOCOL_PREFIXES = [p.lower() + "://" for p in PROTOCOL_CATEGORIES]
# é¢å¤–çš„åè®®åˆ«åå‰ç¼€
ADDITIONAL_PROTOCOL_PREFIXES = {
    "Hysteria2": ["hy2://"],
    "WireGuard": ["wg://"]
}

# --- æ£€æŸ¥éè‹±è¯­æ–‡æœ¬çš„è¾…åŠ©å‡½æ•° ---
def is_non_english_text(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«éè‹±è¯­å­—ç¬¦ï¼ˆå¦‚æ³¢æ–¯è¯­ã€é˜¿æ‹‰ä¼¯è¯­ç­‰ç‰¹æ®Šå­—ç¬¦ï¼‰"""
    if not isinstance(text, str) or not text.strip():
        return False
    
    # å®šä¹‰éæ‹‰ä¸å­—ç¬¦èŒƒå›´ï¼Œä½†æ’é™¤å¸¸è§çš„å›½å®¶åç§°å’Œä»£ç å¯èƒ½ä½¿ç”¨çš„å­—ç¬¦
    # æˆ‘ä»¬éœ€è¦æ›´ç²¾ç¡®åœ°è¯†åˆ«çœŸæ­£éœ€è¦è¿‡æ»¤çš„å­—ç¬¦
    problematic_char_ranges = [
        ('\u0600', '\u06FF'),  # é˜¿æ‹‰ä¼¯è¯­åŠæ³¢æ–¯è¯­
        ('\u0750', '\u077F'),  # é˜¿æ‹‰ä¼¯æ–‡è¡¥å……
        ('\u08A0', '\u08FF'),  # é˜¿æ‹‰ä¼¯æ–‡æ‰©å±•-A
    ]
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜å­—ç¬¦
    for char in text:
        # åªæ£€æŸ¥çœŸæ­£å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—ç¬¦èŒƒå›´
        for start, end in problematic_char_ranges:
            if start <= char <= end:
                return True
    
    # åªè¿‡æ»¤é›¶å®½è¿æ¥ç¬¦ç­‰çœŸæ­£çš„é—®é¢˜å­—ç¬¦
    problematic_chars = ['\u200C', '\u200D']  # é›¶å®½è¿æ¥ç¬¦
    for char in text:
        if char in problematic_chars:
            return True
    
    # ä¿ç•™å¸¸è§çš„å›½å®¶åç§°å­—ç¬¦ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ç­‰
    # è¿™äº›å­—ç¬¦å¯¹äºå›½å®¶è¯†åˆ«å¾ˆé‡è¦ï¼Œä¸åº”è¯¥è¢«è¿‡æ»¤
    return False

# --- Base64 Decoding Helper ---
def decode_base64(data):
    """å®‰å…¨åœ°è§£ç Base64å­—ç¬¦ä¸²ï¼Œå¤„ç†URLå®‰å…¨çš„Base64æ ¼å¼"""
    if not data or not isinstance(data, str):
        return None
    try:
        # æ›¿æ¢URLå®‰å…¨çš„Base64å­—ç¬¦
        data = data.replace('_', '/').replace('-', '+')
        # æ·»åŠ å¿…è¦çš„å¡«å……
        missing_padding = len(data) % 4
        if missing_padding:
            data += '=' * (4 - missing_padding)
        return base64.b64decode(data).decode('utf-8')
    except Exception:
        return None

# --- åè®®åç§°æå–è¾…åŠ©å‡½æ•° ---
def get_vmess_name(vmess_config):
    """
    ä»VMessé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        vmess_config: VMessé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(vmess_config, str) or not vmess_config.startswith('vmess://'):
            return None
        
        # ç§»é™¤å‰ç¼€
        encoded_part = vmess_config[8:]
        
        # å°è¯•è§£ç 
        try:
            # æ·»åŠ å¿…è¦çš„å¡«å……
            padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
            decoded = base64.b64decode(padded).decode('utf-8')
        except Exception:
            # å¦‚æœæ ‡å‡†è§£ç å¤±è´¥ï¼Œå°è¯•URLè§£ç åå†base64è§£ç 
            try:
                encoded_part = unquote(encoded_part)
                padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
                decoded = base64.b64decode(padded).decode('utf-8')
            except Exception:
                return None
        
        # è§£æJSONå¹¶å°è¯•è·å–åç§°
        try:
            vmess_data = json.loads(decoded)
            # å°è¯•ä»ä¸åŒå­—æ®µè·å–åç§°
            for name_field in ['ps', 'name', 'remarks', 'tag']:
                if name_field in vmess_data and isinstance(vmess_data[name_field], str):
                    return vmess_data[name_field].strip()
        except Exception:
            return None
        
        return None
    except Exception:
        return None

def get_ssr_name(ssr_config):
    """
    ä»SSRé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        ssr_config: SSRé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(ssr_config, str) or not ssr_config.startswith('ssr://'):
            return None
        
        # ç§»é™¤å‰ç¼€
        encoded_part = ssr_config[6:]
        
        # å°è¯•è§£ç 
        try:
            # æ·»åŠ å¿…è¦çš„å¡«å……
            padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
            decoded = base64.b64decode(padded).decode('utf-8')
        except Exception:
            # å¦‚æœæ ‡å‡†è§£ç å¤±è´¥ï¼Œå°è¯•URLè§£ç åå†base64è§£ç 
            try:
                encoded_part = unquote(encoded_part)
                padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
                decoded = base64.b64decode(padded).decode('utf-8')
            except Exception:
                return None
        
        # SSRæ ¼å¼: server:port:protocol:method:obfs:password_base64/?params
        parts = decoded.split('/?')
        if len(parts) < 2:
            return None
            
        # è§£æå‚æ•°éƒ¨åˆ†å¹¶è·å–remarks
        params = parse_qs(parts[1])
        if 'remarks' in params:
            try:
                remarks_encoded = params['remarks'][0]
                # è§£ç remarks
                padded_remarks = remarks_encoded + '=' * ((4 - len(remarks_encoded) % 4) % 4)
                return base64.b64decode(padded_remarks).decode('utf-8', errors='ignore')
            except Exception:
                return None
        
        return None
    except Exception:
        return None

def get_trojan_name(trojan_config):
    """
    ä»Trojané…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        trojan_config: Trojané…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(trojan_config, str) or not trojan_config.startswith('trojan://'):
            return None
        
        # Trojan URL æ ¼å¼: trojan://password@hostname:port#name
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in trojan_config:
            try:
                name_part = trojan_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLè·¯å¾„æˆ–æŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = trojan_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_vless_name(vless_config):
    """
    ä»VLESSé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        vless_config: VLESSé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(vless_config, str) or not vless_config.startswith('vless://'):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in vless_config:
            try:
                name_part = vless_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = vless_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_shadowsocks_name(ss_config):
    """
    ä»Shadowsocksé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        ss_config: Shadowsocksé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(ss_config, str) or not ss_config.startswith('ss://'):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in ss_config:
            try:
                name_part = ss_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = ss_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_tuic_name(tuic_config):
    """
    ä»Tuicé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        tuic_config: Tuicé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(tuic_config, str) or not tuic_config.startswith('tuic://'):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in tuic_config:
            try:
                name_part = tuic_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = tuic_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_hysteria2_name(hy2_config):
    """
    ä»Hysteria2é…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        hy2_config: Hysteria2é…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(hy2_config, str):
            return None
        
        # æ”¯æŒhy2://å’Œhysteria2://å‰ç¼€
        config_lower = hy2_config.lower()
        if not (config_lower.startswith('hy2://') or config_lower.startswith('hysteria2://')):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in hy2_config:
            try:
                name_part = hy2_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = hy2_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps', 'tag']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_wireguard_name(wg_config):
    """
    ä»WireGuardé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        wg_config: WireGuardé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(wg_config, str):
            return None
        
        # æ”¯æŒwireguard://å’Œwg://å‰ç¼€
        config_lower = wg_config.lower()
        if not (config_lower.startswith('wireguard://') or config_lower.startswith('wg://')):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in wg_config:
            try:
                name_part = wg_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = wg_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps', 'tag']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        # å¯¹äºbase64ç¼–ç çš„WireGuardé…ç½®ï¼Œå°è¯•è§£ç æŸ¥æ‰¾åç§°
        try:
            prefix = 'wireguard://' if config_lower.startswith('wireguard://') else 'wg://'
            encoded_part = wg_config[len(prefix):]
            
            # å°è¯•è§£ç base64éƒ¨åˆ†
            decoded = decode_base64(encoded_part)
            if decoded:
                # å°è¯•ä»è§£ç åçš„é…ç½®ä¸­æŸ¥æ‰¾åç§°ç›¸å…³ä¿¡æ¯
                for line in decoded.split('\n'):
                    if line.strip().lower().startswith('#'):
                        # æ³¨é‡Šè¡Œå¯èƒ½åŒ…å«åç§°ä¿¡æ¯
                        comment_text = line.strip()[1:].strip()
                        if comment_text:
                            return comment_text
                    elif '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip().lower()
                        value = value.strip().strip('"').strip("'")
                        if key in ['name', 'remarks', 'ps', 'tag', 'description']:
                            return value
        except Exception:
            pass
        
        return None
    except Exception:
        return None

# --- New Filter Function ---
def should_filter_config(config):
    """æ ¹æ®ç‰¹å®šè§„åˆ™è¿‡æ»¤æ— æ•ˆæˆ–ä½è´¨é‡çš„é…ç½®"""
    if not config or not isinstance(config, str):
        return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤çŸ­è¯­
    if FILTERED_PHRASE in config.lower():
        return True
    
    # è¿›ä¸€æ­¥æ”¾å®½URLç¼–ç æ£€æŸ¥ï¼Œå‡å°‘è¯¯åˆ¤
    percent25_count = config.count('%25')
    if percent25_count >= MIN_PERCENT25_COUNT * 4:  # å†æ¬¡æé«˜é˜ˆå€¼
        return True
    
    # è¿›ä¸€æ­¥æ”¾å®½é…ç½®é•¿åº¦é™åˆ¶
    if len(config) >= MAX_CONFIG_LENGTH * 3:  # å†æ¬¡æé«˜é˜ˆå€¼
        return True
    
    # å¢å¼ºçš„åè®®å‰ç¼€æ£€æŸ¥
    has_valid_protocol = False
    config_lower = config.lower()
    
    # æ£€æŸ¥æ ‡å‡†åè®®å‰ç¼€
    for protocol_prefix in PROTOCOL_PREFIXES:
        if protocol_prefix in config_lower:
            has_valid_protocol = True
            break
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…æ ‡å‡†å‰ç¼€ï¼Œæ£€æŸ¥åˆ«åå‰ç¼€
    if not has_valid_protocol:
        for protocol, aliases in ADDITIONAL_PROTOCOL_PREFIXES.items():
            for alias in aliases:
                if alias in config_lower:
                    has_valid_protocol = True
                    break
            if has_valid_protocol:
                break
    
    if not has_valid_protocol:
        return True
    
    return False

async def fetch_url(session, url):
    """å¼‚æ­¥è·å–URLå†…å®¹å¹¶æå–æ–‡æœ¬"""
    try:
        async with session.get(url, timeout=REQUEST_TIMEOUT) as response:
            response.raise_for_status()
            
            # å°è¯•å¤„ç†ä¸åŒçš„å†…å®¹ç±»å‹
            content_type = response.headers.get('Content-Type', '')
            
            # å¦‚æœæ˜¯JSONå†…å®¹ï¼Œç›´æ¥å¤„ç†
            if 'application/json' in content_type:
                try:
                    json_data = await response.json()
                    # å°†JSONè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥æ–¹ä¾¿åç»­å¤„ç†
                    text_content = json.dumps(json_data, ensure_ascii=False)
                    logging.debug(f"å¤„ç†JSONå†…å®¹: {url}")
                except json.JSONDecodeError:
                    # å¦‚æœæ— æ³•è§£æä¸ºJSONï¼Œå›é€€åˆ°æ–‡æœ¬å¤„ç†
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    text_content = soup.get_text(separator='\n', strip=True)
            else:
                # å¤„ç†HTMLæˆ–çº¯æ–‡æœ¬
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ä¼˜å…ˆä»ä»£ç ç›¸å…³æ ‡ç­¾æå–å†…å®¹
                text_content = ""
                code_elements = soup.find_all(['pre', 'code'])
                if code_elements:
                    for element in code_elements:
                        text_content += element.get_text(separator='\n', strip=True) + "\n"
                
                # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„ä»£ç å†…å®¹ï¼Œå†æå–å…¶ä»–æ–‡æœ¬å…ƒç´ 
                if not text_content or len(text_content) < 100:
                    for element in soup.find_all(['p', 'div', 'li', 'span', 'td']):
                        text_content += element.get_text(separator='\n', strip=True) + "\n"
                
                # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
                if not text_content: 
                    text_content = soup.get_text(separator=' ', strip=True)
                    
            logging.info(f"æˆåŠŸè·å–: {url}")
            return url, text_content
    except asyncio.TimeoutError:
        logging.warning(f"Request timed out for {url}")
    except aiohttp.ClientError as e:
        logging.warning(f"Client error fetching {url}: {e}")
    except Exception as e:
        logging.warning(f"Unexpected error fetching {url}: {e}")
    return url, None

def find_matches(text, categories_data):
    """æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾åŒ¹é…é¡¹ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨å¹¶å¢å¼ºåè®®è¯†åˆ«"""
    if not text or not isinstance(text, str):
        return {}
        
    # å¢å¼ºçš„åè®®æ¨¡å¼å®šä¹‰
    PROTOCOL_PATTERNS = {
        "Vmess": [
            r'vmess:\/\/[^ \n\r<"\']+',
            r'vmess:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ],
        "Vless": [
            r'vless:\/\/[^ \n\r<"\']+',
            r'vless:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ],
        "Trojan": [
            r'trojan:\/\/[^ \n\r<"\']+',
            r'trojan:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ],
        "ShadowSocks": [
            r'ss:\/\/[^ \n\r<"\']+',
            r'ss:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ],
        "ShadowSocksR": [
            r'ssr:\/\/[^ \n\r<"\']+',
            r'ssr:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ],
        "Tuic": [
            r'tuic:\/\/[^ \n\r<"\']+',
            r'tuic:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ],
        "Hysteria2": [
            r'hy2:\/\/[^ \n\r<"\']+',
            r'hysteria2:\/\/[^ \n\r<"\']+',
            r'hy2:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*',
            r'hysteria2:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ],
        "WireGuard": [
            r'wireguard:\/\/[^ \n\r<"\']+',
            r'wg:\/\/[^ \n\r<"\']+',
            r'wireguard:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*',
            r'wg:\/\/[a-zA-Z0-9_\-\.\~]+(?:%[0-9a-fA-F]{2})*(?:[a-zA-Z0-9_\-\.\~\/\?\#\[\]\@\!\$\&\'\(\)\*\+\,\;\=\:]|%[0-9a-fA-F]{2})*'
        ]
    }
    
    # åªåˆå§‹åŒ–æœ‰æ¨¡å¼çš„ç±»åˆ«ï¼ŒèŠ‚çœå†…å­˜
    matches = {}
    
    # å¢å¼ºçš„åè®®åŒ¹é…ç­–ç•¥ï¼šå…ˆä½¿ç”¨ç›´æ¥å‰ç¼€åŒ¹é…ï¼Œå†ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
    # 1. é¦–å…ˆä½¿ç”¨ç›´æ¥å­—ç¬¦ä¸²æœç´¢æå–æ‰€æœ‰å¯èƒ½çš„åè®®é“¾æ¥
    all_protocol_matches = {}
    
    # æ”¶é›†æ‰€æœ‰åè®®å‰ç¼€
    all_prefixes = []
    for protocol in PROTOCOL_CATEGORIES:
        all_prefixes.append(protocol.lower() + "://")
    
    # æ·»åŠ åˆ«åå‰ç¼€
    for protocol, aliases in ADDITIONAL_PROTOCOL_PREFIXES.items():
        all_prefixes.extend(aliases)
    
    # ç›´æ¥æœç´¢æ‰€æœ‰åè®®å‰ç¼€
    for prefix in all_prefixes:
        prefix_lower = prefix.lower()
        text_lower = text.lower()
        start_pos = 0
        
        while True:
            pos = text_lower.find(prefix_lower, start_pos)
            if pos == -1:
                break
            
            # æ‰¾åˆ°å‰ç¼€åï¼Œæå–å®Œæ•´çš„URLç›´åˆ°é‡åˆ°ç©ºç™½å­—ç¬¦æˆ–ç»“æŸç¬¦
            end_pos = pos + len(prefix_lower)
            while end_pos < len(text) and text[end_pos] not in [' ', '\n', '\r', '\t', '<', '"', "'"]:
                end_pos += 1
            
            # æå–å®Œæ•´çš„é…ç½®å­—ç¬¦ä¸²
            full_config = text[pos:end_pos]
            if full_config:
                # ç¡®å®šåè®®ç±»åˆ«
                protocol_category = None
                if prefix_lower.startswith('vmess://'):
                    protocol_category = "Vmess"
                elif prefix_lower.startswith('vless://'):
                    protocol_category = "Vless"
                elif prefix_lower.startswith('trojan://'):
                    protocol_category = "Trojan"
                elif prefix_lower.startswith('ss://') and not prefix_lower.startswith('ssr://'):
                    protocol_category = "ShadowSocks"
                elif prefix_lower.startswith('ssr://'):
                    protocol_category = "ShadowSocksR"
                elif prefix_lower.startswith('tuic://'):
                    protocol_category = "Tuic"
                elif prefix_lower.startswith('hy2://') or prefix_lower.startswith('hysteria2://'):
                    protocol_category = "Hysteria2"
                elif prefix_lower.startswith('wireguard://') or prefix_lower.startswith('wg://'):
                    protocol_category = "WireGuard"
                
                if protocol_category:
                    if protocol_category not in all_protocol_matches:
                        all_protocol_matches[protocol_category] = set()
                    all_protocol_matches[protocol_category].add(full_config)
            
            start_pos = end_pos + 1
    
    # 2. ä½¿ç”¨å¢å¼ºçš„æ­£åˆ™è¡¨è¾¾å¼è¿›è¡ŒåŒ¹é…
    for category, patterns in categories_data.items():
        # åªå¤„ç†éç©ºçš„æ¨¡å¼åˆ—è¡¨
        if not patterns or not isinstance(patterns, list):
            continue
            
        category_matches = set()
        
        # å¦‚æœå·²ç»æœ‰ç›´æ¥åŒ¹é…çš„ç»“æœï¼Œå…ˆæ·»åŠ å®ƒä»¬
        if category in all_protocol_matches:
            category_matches.update(all_protocol_matches[category])
        
        # æ·»åŠ å†…ç½®çš„åè®®æ¨¡å¼
        if category in PROTOCOL_PATTERNS:
            for pattern_str in PROTOCOL_PATTERNS[category]:
                try:
                    pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE | re.DOTALL)
                    found = pattern.findall(text)
                    if found:
                        for item in found:
                            if item and isinstance(item, str):
                                cleaned_item = item.strip()
                                if cleaned_item:
                                    category_matches.add(cleaned_item)
                except re.error as e:
                    logging.error(f"å†…ç½®æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯ - æ¨¡å¼ '{pattern_str}' åœ¨ç±»åˆ« '{category}': {e}")
                    continue
        
        # ç„¶åå¤„ç†ç”¨æˆ·æä¾›çš„æ¨¡å¼
        for pattern_str in patterns:
            if not isinstance(pattern_str, str):
                continue
                
            try:
                # ä½¿ç”¨é¢„ç¼–è¯‘çš„åè®®å‰ç¼€åˆ—è¡¨æé«˜æ€§èƒ½
                is_protocol_pattern = any(proto_prefix in pattern_str.lower() for proto_prefix in PROTOCOL_PREFIXES)
                
                if category in PROTOCOL_CATEGORIES or is_protocol_pattern:
                    # ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½
                    pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE | re.DOTALL)
                    found = pattern.findall(text)
                    
                    if found:
                        # æ¸…ç†å¹¶å»é‡åŒ¹é…ç»“æœ
                        for item in found:
                            if item and isinstance(item, str):
                                cleaned_item = item.strip()
                                if cleaned_item:
                                    category_matches.add(cleaned_item)
            except re.error as e:
                logging.error(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯ - æ¨¡å¼ '{pattern_str}' åœ¨ç±»åˆ« '{category}': {e}")
                continue
        
        if category_matches:
            matches[category] = category_matches
    
    # åªè¿”å›éç©ºçš„åŒ¹é…ç»“æœ
    return {k: v for k, v in matches.items() if v}

def save_to_file(directory, category_name, items_set):
    """å°†é¡¹ç›®é›†åˆä¿å­˜åˆ°æŒ‡å®šç›®å½•çš„æ–‡æœ¬æ–‡ä»¶ä¸­"""
    if not items_set:
        logging.debug(f"è·³è¿‡ç©ºé›†åˆçš„ä¿å­˜: {category_name}")
        return False, 0
        
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    try:
        os.makedirs(directory, exist_ok=True)
        file_path = os.path.join(directory, f"{category_name}.txt")
        count = len(items_set)
        
        # å†™å…¥æ’åºåçš„é¡¹ç›®ï¼Œæ¯è¡Œä¸€ä¸ª
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in sorted(list(items_set)): 
                f.write(f"{item}\n")
        
        logging.info(f"å·²ä¿å­˜ {count} é¡¹åˆ° {file_path}")
        return True, count
    except IOError as e:
        logging.error(f"å†™å…¥æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    except Exception as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ {file_path}: {e}")
    return False, 0

# --- ä½¿ç”¨æ——å¸œå›¾åƒç”Ÿæˆç®€å•çš„READMEå‡½æ•° ---
def generate_simple_readme(protocol_counts, country_counts, all_keywords_data, use_local_paths=True):
    """ç”ŸæˆREADME.mdæ–‡ä»¶ï¼Œå±•ç¤ºæŠ“å–ç»“æœç»Ÿè®¡ä¿¡æ¯"""
    # ç¡®ä¿è¾“å…¥å‚æ•°æ˜¯å­—å…¸ç±»å‹
    if not isinstance(protocol_counts, dict):
        protocol_counts = {}
    if not isinstance(country_counts, dict):
        country_counts = {}
    
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S %Z")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_protocol_configs = sum(protocol_counts.values())
    total_country_configs = sum(country_counts.values())
    countries_with_data = len(country_counts)
    protocols_with_data = len(protocol_counts)

    # æ„å»ºå­ç›®å½•çš„è·¯å¾„
    if use_local_paths:
        protocol_base_url = f"{OUTPUT_DIR}/{PROTOCOL_SUBDIR}"
        country_base_url = f"{OUTPUT_DIR}/{COUNTRY_SUBDIR}"
    else:
        # ä¿ç•™GitHubè¿œç¨‹è·¯å¾„æ”¯æŒä½œä¸ºå¤‡ç”¨
        github_repo_path = "miladtahanian/V2RayScrapeByCountry"
        github_branch = "main"
        protocol_base_url = f"https://raw.githubusercontent.com/{github_repo_path}/refs/heads/{github_branch}/{OUTPUT_DIR}/{PROTOCOL_SUBDIR}"
        country_base_url = f"https://raw.githubusercontent.com/{github_repo_path}/refs/heads/{github_branch}/{OUTPUT_DIR}/{COUNTRY_SUBDIR}"

    md_content = f"# ğŸ“Š æå–ç»“æœ (æœ€åæ›´æ–°: {timestamp})\n\n"
    md_content += "æ­¤æ–‡ä»¶æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ã€‚\n\n"
    md_content += f"## ğŸ“‹ ç»Ÿè®¡æ¦‚è§ˆ\n\n"
    md_content += f"- **é…ç½®æ€»æ•°**: {total_protocol_configs}\n"
    md_content += f"- **æœ‰æ•°æ®çš„åè®®æ•°**: {protocols_with_data}\n"
    md_content += f"- **å›½å®¶ç›¸å…³é…ç½®æ•°**: {total_country_configs}\n"
    md_content += f"- **æœ‰é…ç½®çš„å›½å®¶æ•°**: {countries_with_data}\n\n"
    
    md_content += "## â„¹ï¸ è¯´æ˜\n\n"
    md_content += "å›½å®¶æ–‡ä»¶ä»…åŒ…å«åœ¨**é…ç½®åç§°**ä¸­æ‰¾åˆ°å›½å®¶åç§°/æ——å¸œçš„é…ç½®ã€‚é…ç½®åç§°é¦–å…ˆä»é“¾æ¥çš„`#`éƒ¨åˆ†æå–ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä»å†…éƒ¨åç§°(å¯¹äºVmess/SSR)æå–ã€‚\n\n"
    md_content += "è¿‡åº¦URLç¼–ç çš„é…ç½®(åŒ…å«å¤§é‡`%25`ã€è¿‡é•¿æˆ–åŒ…å«ç‰¹å®šå…³é”®è¯çš„)å·²ä»ç»“æœä¸­åˆ é™¤ã€‚\n\n"
    md_content += "æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²æŒ‰ç±»åˆ«æ•´ç†åˆ°ä¸åŒç›®å½•ä¸­ï¼Œä¾¿äºæŸ¥æ‰¾å’Œä½¿ç”¨ã€‚\n\n"

    md_content += "## ğŸ“ åè®®æ–‡ä»¶\n\n"
    if protocol_counts:
        md_content += "| åè®® | æ€»æ•° | é“¾æ¥ |\n"
        md_content += "|---|---|---|\n"
        for category_name, count in sorted(protocol_counts.items()):
            file_link = f"{protocol_base_url}/{category_name}.txt"
            md_content += f"| {category_name} | {count} | [`{category_name}.txt`]({file_link}) |\n"
    else:
        md_content += "æ²¡æœ‰æ‰¾åˆ°åè®®é…ç½®ã€‚\n"
    md_content += "\n"

    md_content += "## ğŸŒ å›½å®¶æ–‡ä»¶ (åŒ…å«é…ç½®)\n\n"
    if country_counts:
        md_content += "| å›½å®¶ | ç›¸å…³é…ç½®æ•°é‡ | é“¾æ¥ |\n"
        md_content += "|---|---|---|\n"
        for country_category_name, count in sorted(country_counts.items()):
            flag_image_markdown = "" # ç”¨äºä¿å­˜æ——å¸œå›¾åƒHTMLæ ‡ç­¾
            
            # æŸ¥æ‰¾å›½å®¶çš„ä¸¤å­—æ¯ISOä»£ç ç”¨äºæ——å¸œå›¾åƒURL
            if country_category_name in all_keywords_data:
                keywords_list = all_keywords_data[country_category_name]
                if keywords_list and isinstance(keywords_list, list):
                    for item in keywords_list:
                        if isinstance(item, str) and len(item) == 2 and item.isupper() and item.isalpha():
                            iso_code_lowercase_for_url = item.lower()
                            # ä½¿ç”¨flagcdn.comï¼Œå®½åº¦ä¸º20åƒç´ 
                            flag_image_url = f"https://flagcdn.com/w20/{iso_code_lowercase_for_url}.png"
                            flag_image_markdown = f'<img src="{flag_image_url}" width="20" alt="{country_category_name} flag">'
                            break 

            # ä¸º"å›½å®¶"åˆ—æ„å»ºæœ€ç»ˆæ–‡æœ¬
            display_parts = []
            # å¦‚æœæ——å¸œå›¾åƒæ ‡ç­¾å·²åˆ›å»º
            if flag_image_markdown:
                display_parts.append(flag_image_markdown)
            
            display_parts.append(country_category_name) # åŸå§‹åç§° (é”®)
            
            country_display_text = " ".join(display_parts)
            
            file_link = f"{country_base_url}/{country_category_name}.txt"
            link_text = f"{country_category_name}.txt"
            md_content += f"| {country_display_text} | {count} | [`{link_text}`]({file_link}) |\n"
    else:
        md_content += "æ²¡æœ‰æ‰¾åˆ°ä¸å›½å®¶ç›¸å…³çš„é…ç½®ã€‚\n"
    md_content += "\n"

    try:
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"æˆåŠŸç”Ÿæˆ {README_FILE}")
    except Exception as e:
        logging.error(f"å†™å…¥ {README_FILE} å¤±è´¥: {e}")

# mainå‡½æ•°å’Œå…¶ä»–å‡½æ•°å®ç°
async def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªæŠ“å–å’Œå¤„ç†æµç¨‹"""
    # ç¡®ä¿é…ç½®æ–‡ä»¶å¤¹å­˜åœ¨
    try:
        os.makedirs(CONFIG_DIR, exist_ok=True)
    except Exception as e:
        logging.error(f"åˆ›å»ºé…ç½®æ–‡ä»¶å¤¹ '{CONFIG_DIR}' å¤±è´¥: {e}")
    
    # æ£€æŸ¥å¿…è¦çš„è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(URLS_FILE) or not os.path.exists(KEYWORDS_FILE):
        missing_files = []
        if not os.path.exists(URLS_FILE):
            missing_files.append(f"URLsæ–‡ä»¶: {URLS_FILE}")
        if not os.path.exists(KEYWORDS_FILE):
            missing_files.append(f"å…³é”®è¯æ–‡ä»¶: {KEYWORDS_FILE}")
        
        logging.critical(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶:\n- {chr(10)}- ".join(missing_files))
        logging.info(f"è¯·ç¡®ä¿è¿™äº›æ–‡ä»¶å·²æ”¾åœ¨ {CONFIG_DIR} æ–‡ä»¶å¤¹ä¸­")
        return

    # åŠ è½½URLå’Œå…³é”®è¯æ•°æ®
    try:
        with open(URLS_FILE, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
            
        if not urls:
            logging.critical("URLsæ–‡ä»¶ä¸ºç©ºï¼Œæ²¡æœ‰è¦æŠ“å–çš„URLã€‚")
            return
            
        logging.info(f"å·²ä» {URLS_FILE} åŠ è½½ {len(urls)} ä¸ªURL")
        
        with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
            categories_data = json.load(f)
            
        # éªŒè¯categories_dataæ˜¯å­—å…¸ç±»å‹
        if not isinstance(categories_data, dict):
            logging.critical("keywords.jsonå¿…é¡»åŒ…å«å­—å…¸æ ¼å¼çš„æ•°æ®ã€‚")
            return
            
        # éªŒè¯åè®®ç±»åˆ«æ˜¯å¦åœ¨é…ç½®ä¸­
        missing_protocols = [p for p in PROTOCOL_CATEGORIES if p not in categories_data]
        if missing_protocols:
            logging.warning(f"keywords.jsonä¸­ç¼ºå°‘ä»¥ä¸‹åè®®ç±»åˆ«çš„é…ç½®: {', '.join(missing_protocols)}")
            
        # éªŒè¯æ¯ä¸ªå€¼éƒ½æ˜¯åˆ—è¡¨
        invalid_entries = [(k, v) for k, v in categories_data.items() if not isinstance(v, list)]
        if invalid_entries:
            logging.warning(f"keywords.jsonåŒ…å«éåˆ—è¡¨æ ¼å¼çš„å€¼: {invalid_entries}")
            # è¿‡æ»¤æ‰éåˆ—è¡¨çš„å€¼
            categories_data = {k: v for k, v in categories_data.items() if isinstance(v, list)}
            
        if not categories_data:
            logging.critical("keywords.jsonä¸­æ²¡æœ‰æœ‰æ•ˆçš„ç±»åˆ«æ•°æ®ã€‚")
            return
            
    except json.JSONDecodeError as e:
        logging.critical(f"è§£ækeywords.jsonæ–‡ä»¶å¤±è´¥: {e}")
        return
    except IOError as e:
        logging.critical(f"è¯»å–è¾“å…¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # å®šä¹‰å¢å¼ºçš„åè®®æ¨¡å¼
    PROTOCOL_PATTERNS = {
        "Vmess": [r'vmess://[^\s]+'],
        "Vless": [r'vless://[^\s]+'],
        "Trojan": [r'trojan://[^\s]+'],
        "ShadowSocks": [r'ss://[^\s]+(?!r://)'],
        "ShadowSocksR": [r'ssr://[^\s]+'],
        "Tuic": [r'tuic://[^\s]+'],
        "Hysteria2": [r'(?:hysteria2|hy2)://[^\s]+'],
        "WireGuard": [r'(?:wireguard|wg)://[^\s]+']
    }
    
    # ä½¿ç”¨å¢å¼ºçš„åè®®æ¨¡å¼è¿›è¡ŒåŒ¹é…ï¼Œä»…ä»keywords.jsonä¸­åŠ è½½å›½å®¶å…³é”®è¯
    protocol_patterns_for_matching = PROTOCOL_PATTERNS
    country_keywords_for_naming = {
        cat: patterns for cat, patterns in categories_data.items() if cat not in PROTOCOL_CATEGORIES
    }
    country_category_names = list(country_keywords_for_naming.keys())

    logging.info(f"å·²åŠ è½½ {len(urls)} ä¸ªURLå’Œ "
                 f"{len(categories_data)} ä¸ªæ€»ç±»åˆ«ä»keywords.jsonã€‚")

    # å¼‚æ­¥è·å–æ‰€æœ‰é¡µé¢
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)  # é™åˆ¶å¹¶å‘è¯·æ±‚æ•°
    
    async def fetch_with_semaphore(session, url_to_fetch):
        """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘çš„fetch_url"""
        async with sem:
            return await fetch_url(session, url_to_fetch)
    
    # åˆ›å»ºHTTPä¼šè¯å¹¶æ‰§è¡Œæ‰€æœ‰è·å–ä»»åŠ¡
    async with aiohttp.ClientSession() as session:
        logging.info(f"å¼€å§‹è·å– {len(urls)} ä¸ªURLs (æœ€å¤§å¹¶å‘: {CONCURRENT_REQUESTS})...")
        fetched_pages = await asyncio.gather(
            *[fetch_with_semaphore(session, u) for u in urls],
            return_exceptions=True  # å³ä½¿æŸäº›ä»»åŠ¡å¤±è´¥ä¹Ÿç»§ç»­æ‰§è¡Œ
        )
        
        # è¿‡æ»¤å‡ºæˆåŠŸè·å–çš„é¡µé¢å¹¶ç»Ÿè®¡å¤±è´¥æƒ…å†µ
        success_count = 0
        exception_count = 0
        filtered_pages = []
        
        for result in fetched_pages:
            if isinstance(result, tuple) and len(result) == 2 and isinstance(result[0], str) and result[1] is not None:
                filtered_pages.append(result)
                success_count += 1
            elif isinstance(result, Exception):
                exception_count += 1
                logging.warning(f"URLè·å–ä»»åŠ¡å¼‚å¸¸: {type(result).__name__}: {result}")
            else:
                logging.debug(f"æ— æ•ˆçš„URLè·å–ç»“æœ: {type(result)}")
        
        fetched_pages = filtered_pages
        logging.info(f"URLè·å–å®Œæˆ: æˆåŠŸ {success_count}, å¼‚å¸¸ {exception_count}, æ€»è®¡ {len(filtered_pages)} ä¸ªé¡µé¢å¾…å¤„ç†")

    # åˆå§‹åŒ–ç»“æœé›†åˆ
    final_configs_by_country = {cat: set() for cat in country_category_names}
    final_all_protocols = {cat: set() for cat in PROTOCOL_CATEGORIES}

    logging.info("å¤„ç†é¡µé¢å¹¶å…³è”é…ç½®åç§°...")
    
    # ç»Ÿè®¡æˆåŠŸå¤„ç†çš„é¡µé¢æ•°é‡
    processed_pages = 0
    found_configs = 0
    filtered_out_configs = 0
    
    for url, text in fetched_pages:
        if not text:
            continue
            
        processed_pages += 1
        page_protocol_matches = find_matches(text, protocol_patterns_for_matching)
        all_page_configs_after_filter = set()
        
        # å¤„ç†æ‰¾åˆ°çš„åè®®é…ç½®
        page_filtered_count = 0
        for protocol_cat_name, configs_found in page_protocol_matches.items():
            if protocol_cat_name in PROTOCOL_CATEGORIES:
                for config in configs_found:
                    if not should_filter_config(config):
                        all_page_configs_after_filter.add(config)
                        final_all_protocols[protocol_cat_name].add(config)
                    else:
                        page_filtered_count += 1
        
        found_configs += len(all_page_configs_after_filter)
        filtered_out_configs += page_filtered_count
        
        # æ¯10ä¸ªé¡µé¢è¾“å‡ºä¸€æ¬¡è¿›åº¦
        if processed_pages % 10 == 0:
            logging.info(f"å¤„ç†è¿›åº¦: {processed_pages}/{len(fetched_pages)} é¡µé¢, " \
                      f"å·²æ‰¾åˆ° {found_configs} é…ç½®, å·²è¿‡æ»¤ {filtered_out_configs} é…ç½®")

        # ä¸ºæ¯ä¸ªé…ç½®å…³è”å›½å®¶ä¿¡æ¯
        for config in all_page_configs_after_filter:
            name_to_check = None
            
            # 1. é¦–å…ˆå°è¯•ä»URLç‰‡æ®µä¸­æå–åç§°ï¼ˆ#åé¢çš„éƒ¨åˆ†ï¼‰
            if '#' in config:
                try:
                    potential_name = config.split('#', 1)[1]
                    name_to_check = unquote(potential_name).strip()
                    if not name_to_check:
                        name_to_check = None
                except (IndexError, Exception) as e:
                    logging.debug(f"ä»URLç‰‡æ®µæå–åç§°å¤±è´¥: {e}")

            # 2. å¦‚æœURLç‰‡æ®µä¸­æ²¡æœ‰åç§°ï¼Œå°è¯•ä»åè®®ç‰¹å®šå­—æ®µæå–
        if not name_to_check:
            config_lower = config.lower()
            if config_lower.startswith('ssr://'):
                name_to_check = get_ssr_name(config)
            elif config_lower.startswith('vmess://'):
                name_to_check = get_vmess_name(config)
            elif config_lower.startswith('trojan://'):
                name_to_check = get_trojan_name(config)
            elif config_lower.startswith('vless://'):
                name_to_check = get_vless_name(config)
            elif config_lower.startswith('ss://'):
                name_to_check = get_shadowsocks_name(config)
            elif config_lower.startswith('tuic://'):
                name_to_check = get_tuic_name(config)
            elif config_lower.startswith('hy2://') or config_lower.startswith('hysteria2://'):
                name_to_check = get_hysteria2_name(config)
            elif config_lower.startswith('wireguard://') or config_lower.startswith('wg://'):
                name_to_check = get_wireguard_name(config)
            # æ‰€æœ‰åè®®éƒ½æœ‰åç§°æå–æ”¯æŒäº†

            # å¦‚æœæ— æ³•è·å–åç§°ï¼Œè·³è¿‡æ­¤é…ç½®
            if not name_to_check or not isinstance(name_to_check, str):
                continue
                
            current_name_to_check_str = name_to_check.strip()

            # éå†æ¯ä¸ªå›½å®¶çš„å…³é”®è¯åˆ—è¡¨ï¼Œå¯»æ‰¾åŒ¹é…
            country_matched = False
            for country_name_key, keywords_for_country_list in country_keywords_for_naming.items():
                # åªå¤„ç†æœ‰æ•ˆçš„å…³é”®è¯åˆ—è¡¨
                if not isinstance(keywords_for_country_list, list):
                    continue
                    
                # å‡†å¤‡æ­¤å›½å®¶çš„æ–‡æœ¬å…³é”®è¯ï¼Œä¿ç•™æ‰€æœ‰æœ‰æ•ˆçš„å…³é”®è¯
                text_keywords_for_country = []
                for kw in keywords_for_country_list:
                    if isinstance(kw, str) and kw.strip():
                        # ç§»é™¤è¿‡åº¦çš„è¿‡æ»¤ï¼Œåªè¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å’Œçº¯è¡¨æƒ…ç¬¦å·
                        # å…è®¸æ‰€æœ‰æœ‰æ•ˆçš„å›½å®¶å…³é”®è¯ï¼ŒåŒ…æ‹¬éè‹±è¯­å­—ç¬¦
                        if len(kw.strip()) > 0:
                            # åªæ·»åŠ å”¯ä¸€çš„å…³é”®è¯
                            if kw not in text_keywords_for_country:
                                text_keywords_for_country.append(kw)
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å…³é”®è¯
                match_found = False
                current_name_lower = current_name_to_check_str.lower()
                
                # æ·»åŠ è°ƒè¯•æ—¥å¿—
                if processed_pages % 50 == 0:
                    logging.debug(f"å¤„ç†é…ç½®åç§°: '{current_name_to_check_str}' é•¿åº¦: {len(current_name_to_check_str)}")
                
                for keyword in text_keywords_for_country:
                    if not isinstance(keyword, str):
                        continue
                        
                    # ç§»é™¤å…³é”®è¯å‰åç©ºæ ¼
                    keyword = keyword.strip()
                    if not keyword:
                        continue
                        
                    # å¯¹ç¼©å†™ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œå¯¹æ™®é€šè¯ä½¿ç”¨åŒ…å«åŒ¹é…
                    is_abbr = (len(keyword) in [2, 3]) and keyword.isupper() and keyword.isalpha()
                    keyword_lower = keyword.lower()
                    
                    if is_abbr:
                        # å¯¹äºç¼©å†™ï¼Œä½¿ç”¨æ›´çµæ´»çš„åŒ¹é…ç­–ç•¥
                        try:
                            # å°è¯•ç²¾ç¡®åŒ¹é…ç¼©å†™
                            pattern = r'\b' + re.escape(keyword) + r'\b'
                            if re.search(pattern, current_name_to_check_str, re.IGNORECASE):
                                match_found = True
                                logging.debug(f"å›½å®¶'{country_name_key}' åŒ¹é…ç¼©å†™: '{keyword}'")
                                break
                            # å°è¯•å¦ä¸€ç§æ–¹å¼ï¼šåœ¨é…ç½®åç§°ä¸­æŸ¥æ‰¾å›½å®¶ä»£ç ï¼Œå…è®¸å‰åæœ‰éå­—æ¯å­—ç¬¦
                            if keyword_lower in current_name_lower:
                                # æ£€æŸ¥æ˜¯å¦æ˜¯ç‹¬ç«‹çš„å›½å®¶ä»£ç ï¼Œé¿å…åŒ¹é…åˆ°å…¶ä»–å•è¯ä¸­åŒ…å«çš„å­—æ¯ç»„åˆ
                                parts = re.split(r'[^a-zA-Z]', current_name_to_check_str.lower())
                                if keyword_lower in parts:
                                    match_found = True
                                    logging.debug(f"å›½å®¶'{country_name_key}' åŒ¹é…åˆ†å‰²åç¼©å†™: '{keyword}'")
                                    break
                        except Exception:
                            # é™é»˜è·³è¿‡æ­£åˆ™åŒ¹é…é”™è¯¯
                            pass
                    else:
                        # å¯¹äºæ™®é€šå…³é”®è¯ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…
                        # å¯¹äºå¤šè¯­è¨€å…³é”®è¯ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…ç­–ç•¥
                        if not is_non_english_text(keyword):
                            # è‹±è¯­å…³é”®è¯ä½¿ç”¨ä¸¥æ ¼çš„åŒ…å«æ£€æŸ¥
                            if keyword_lower in current_name_lower:
                                match_found = True
                                logging.debug(f"å›½å®¶'{country_name_key}' åŒ¹é…è‹±è¯­å…³é”®è¯: '{keyword}'")
                                break
                        else:
                            # éè‹±è¯­å…³é”®è¯ä½¿ç”¨ç›´æ¥æ¯”è¾ƒ
                            if keyword in current_name_to_check_str or keyword_lower in current_name_lower:
                                match_found = True
                                logging.debug(f"å›½å®¶'{country_name_key}' åŒ¹é…éè‹±è¯­å…³é”®è¯: '{keyword}'")
                                break
                
                if match_found:
                    final_configs_by_country[country_name_key].add(config)
                    country_matched = True
                    logging.debug(f"é…ç½®å·²å…³è”åˆ°å›½å®¶: {country_name_key}")
                    # ç§»é™¤è¿™é‡Œçš„breakï¼Œå…è®¸é…ç½®åŒ¹é…å¤šä¸ªå›½å®¶
                
            # ç§»é™¤è¿™é‡Œçš„breakï¼Œç¡®ä¿æ¯ä¸ªé…ç½®éƒ½èƒ½è¢«å®Œå…¨å¤„ç†

    # ç»Ÿè®¡ä¿¡æ¯æ—¥å¿—
    logging.info(f"æˆåŠŸå¤„ç† {processed_pages}/{len(fetched_pages)} ä¸ªé¡µé¢ï¼Œæ‰¾åˆ° {found_configs} ä¸ªæœ‰æ•ˆé…ç½®ï¼Œè¿‡æ»¤æ‰ {filtered_out_configs} ä¸ªæ— æ•ˆé…ç½®")
    
    # ç¡®ä¿åˆ é™¤ä»»ä½•å¯èƒ½çš„æ—§å›½å®¶è®¡æ•°æ•°æ®ï¼Œé‡æ–°åŸºäºé›†åˆå¤§å°è®¡ç®—
    country_counts = {}
    
    # å›½å®¶è®¡æ•°å°†åœ¨ä¿å­˜æ–‡ä»¶æ—¶åŸºäºé›†åˆå¤§å°è®¡ç®—ï¼Œæ­¤å¤„åˆ é™¤é‡å¤ä»£ç 
    
    # å‡†å¤‡è¾“å‡ºç›®å½•ç»“æ„
    country_dir = os.path.join(OUTPUT_DIR, COUNTRY_SUBDIR)
    protocol_dir = os.path.join(OUTPUT_DIR, PROTOCOL_SUBDIR)
    
    if os.path.exists(OUTPUT_DIR):
        try:
            shutil.rmtree(OUTPUT_DIR)
            logging.info(f"å·²åˆ é™¤æ—§çš„è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        except (PermissionError, OSError) as e:
            logging.warning(f"æ— æ³•åˆ é™¤æ—§è¾“å‡ºç›®å½•: {e}ï¼Œå°è¯•ä½¿ç”¨æ–°ç›®å½•å")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = f"{OUTPUT_DIR}_backup_{timestamp}"
            try:
                shutil.move(OUTPUT_DIR, backup_dir)
                logging.info(f"å·²å°†æ—§ç›®å½•é‡å‘½åä¸º: {backup_dir}")
            except Exception as inner_e:
                logging.error(f"é‡å‘½åæ—§ç›®å½•å¤±è´¥: {inner_e}")
                # ç»§ç»­æ‰§è¡Œï¼Œè®©os.makedirså¤„ç†å¯èƒ½çš„ç›®å½•å­˜åœ¨æƒ…å†µ
    
    # ç¡®ä¿è¾“å‡ºç›®å½•ç»“æ„å­˜åœ¨
    try:
        os.makedirs(country_dir, exist_ok=True)
        os.makedirs(protocol_dir, exist_ok=True)
        logging.info(f"æ­£åœ¨ä¿å­˜æ–‡ä»¶åˆ°ç›®å½•: {OUTPUT_DIR}")
        logging.info(f"å›½å®¶é…ç½®å°†ä¿å­˜åˆ°: {country_dir}")
        logging.info(f"åè®®é…ç½®å°†ä¿å­˜åˆ°: {protocol_dir}")
    except (PermissionError, OSError) as e:
        logging.critical(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {e}")
        return

    # ä¿å­˜åè®®é…ç½®æ–‡ä»¶
    protocol_counts = {}
    for category, items in final_all_protocols.items():
        if items:  # åªä¿å­˜éç©ºé›†åˆ
            saved, count = save_to_file(protocol_dir, category, items)
            if saved:
                protocol_counts[category] = count
    
    # ä¿å­˜å›½å®¶é…ç½®æ–‡ä»¶å¹¶ç¡®ä¿è®¡æ•°å‡†ç¡®
    country_counts = {}
    countries_with_configs = 0
    total_country_configs = 0
    
    for category, items in final_configs_by_country.items():
        if items:  # åªä¿å­˜éç©ºé›†åˆ
            # ç¡®ä¿ä½¿ç”¨é›†åˆçš„å®é™…å¤§å°ä½œä¸ºè®¡æ•°
            actual_count = len(items)
            saved, count = save_to_file(country_dir, category, items)
            if saved:
                country_counts[category] = actual_count
                countries_with_configs += 1
                total_country_configs += actual_count
                logging.debug(f"å·²ä¿å­˜å›½å®¶é…ç½®: {category}, èŠ‚ç‚¹æ•°é‡: {actual_count}")
    
    # ç”ŸæˆREADMEæ–‡ä»¶
    try:
        generate_simple_readme(protocol_counts, country_counts, categories_data, use_local_paths=True)
    except Exception as e:
        logging.error(f"ç”ŸæˆREADMEæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        # ç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­ç¨‹åº
    
    # è¾“å‡ºå®Œæˆä¿¡æ¯
    logging.info(f"=== æŠ“å–å®Œæˆ ===")
    logging.info(f"æ‰¾åˆ°å¹¶ä¿å­˜çš„åè®®é…ç½®: {sum(protocol_counts.values())}")
    logging.info(f"æœ‰é…ç½®çš„å›½å®¶æ•°é‡: {countries_with_configs}")
    logging.info(f"å›½å®¶ç›¸å…³é…ç½®æ€»æ•°: {total_country_configs}")
    logging.info(f"è¾“å‡ºç›®å½•ç»“æ„:")
    logging.info(f"- åè®®é…ç½®: {os.path.join(OUTPUT_DIR, PROTOCOL_SUBDIR)}")
    logging.info(f"- å›½å®¶é…ç½®: {os.path.join(OUTPUT_DIR, COUNTRY_SUBDIR)}")
    logging.info(f"READMEæ–‡ä»¶å·²æ›´æ–°: {README_FILE}")

if __name__ == "__main__":
    try:
        logging.info("=== V2Rayé…ç½®æŠ“å–å·¥å…·å¼€å§‹è¿è¡Œ ===")
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logging.critical(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        logging.info("=== ç¨‹åºç»“æŸ ===")
