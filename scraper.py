# æ ‡å‡†åº“å¯¼å…¥
import asyncio
import aiohttp
import json
import re
import logging
import os
import shutil


from datetime import datetime
import pytz
import base64

from urllib.parse import parse_qs, unquote

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import psutil  # ç”¨äºå†…å­˜ç›‘æ§

# BeautifulSoupæš‚æ—¶ä¿ç•™ï¼Œå¯èƒ½åœ¨åç»­åŠŸèƒ½æ‰©å±•ä¸­ä½¿ç”¨
from bs4 import BeautifulSoup

# --- é…ç½®å¸¸é‡ ---
# ä½¿ç”¨ç»å¯¹è·¯å¾„ä»¥é¿å…è·¯å¾„è§£æé—®é¢˜
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_DIR = os.path.join(BASE_DIR, 'config')  # é…ç½®æ–‡ä»¶å¤¹ï¼Œç”¨äºå­˜æ”¾è¾“å…¥æ–‡ä»¶
URLS_FILE = os.path.join(CONFIG_DIR, 'urls.txt')
KEYWORDS_FILE = os.path.join(CONFIG_DIR, 'keywords.json') # åº”åŒ…å«å›½å®¶çš„ä¸¤å­—æ¯ä»£ç 
OUTPUT_DIR = os.path.join(BASE_DIR, 'output_configs')  # ä½¿ç”¨ç»å¯¹è·¯å¾„
COUNTRY_SUBDIR = 'countries'  # å›½å®¶é…ç½®æ–‡ä»¶å¤¹
PROTOCOL_SUBDIR = 'protocols' # åè®®é…ç½®æ–‡ä»¶å¤¹
README_FILE = os.path.join(BASE_DIR, 'README.md')  # ä½¿ç”¨ç»å¯¹è·¯å¾„

# è¿è¡Œæ—¶é…ç½®
REQUEST_TIMEOUT = 15  # HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
CONCURRENT_REQUESTS = 10  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
MAX_CONFIG_LENGTH = 1500  # é…ç½®æœ€å¤§é•¿åº¦
MIN_PERCENT25_COUNT = 15  # æœ€å°%25å‡ºç°æ¬¡æ•°ï¼ˆç”¨äºæ£€æµ‹è¿‡åº¦URLç¼–ç ï¼‰
FILTERED_PHRASE = 'i_love_'  # è¦è¿‡æ»¤çš„ç‰¹å®šçŸ­è¯­

# æ€§èƒ½ä¼˜åŒ–è®¾ç½®
MAX_PAGE_SIZE = 5 * 1024 * 1024  # æœ€å¤§é¡µé¢å¤§å°(5MB)ï¼Œé˜²æ­¢è¿‡å¤§çš„é¡µé¢æ¶ˆè€—è¿‡å¤šå†…å­˜
MAX_TOTAL_CONFIGS = 100000  # æœ€å¤§æ€»é…ç½®æ•°é‡ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º

# åè®®å‰ç¼€å¸¸é‡ - ç¡®ä¿é”®åä¸PROTOCOL_CATEGORIESä¸€è‡´
PROTOCOL_PREFIXES = {
    'vmess': ['vmess://'],
    'vless': ['vless://'],
    'trojan': ['trojan://'],
    'shadowsocks': ['ss://'],
    'shadowsocksr': ['ssr://'],
    'wireguard': ['wg://', 'wireguard://'],
    'tuic': ['tuic://'],
    'hysteria2': ['hy2://', 'hysteria2://']
}

# --- Logging Setup ---
# åˆ›å»ºæ—¥å¿—ç›®å½•
LOG_DIR = os.path.join(BASE_DIR, 'logs')
os.makedirs(LOG_DIR, exist_ok=True)

# ç”Ÿæˆå¸¦æœ‰æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶å
log_filename = datetime.now().strftime("%Y%m%d_%H%M%S_scraper.log")
log_file_path = os.path.join(LOG_DIR, log_filename)

# åˆ›å»ºloggerå®ä¾‹
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
logger.handlers.clear()

# åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
file_handler.setLevel(logging.DEBUG)  # æ–‡ä»¶è®°å½•æ‰€æœ‰çº§åˆ«çš„æ—¥å¿—

# è®¾ç½®æ ¼å¼å™¨
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨åˆ°logger
logger.addHandler(console_handler)
logger.addHandler(file_handler)

# è¦†ç›–é»˜è®¤çš„loggingæ¨¡å—ï¼Œä½¿æ‰€æœ‰è°ƒç”¨ä½¿ç”¨æˆ‘ä»¬çš„logger
logging = logger

# ---# åè®®ç±»åˆ« - ä¸PROTOCOL_PREFIXESå’ŒPROTOCOL_REGEX_PATTERNSä¿æŒä¸€è‡´
PROTOCOL_CATEGORIES = [
    "Vmess", "Vless", "Trojan", "ShadowSocks", "ShadowSocksR",
    "WireGuard", "Tuic", "Hysteria2"
] # --- æ£€æŸ¥éè‹±è¯­æ–‡æœ¬çš„è¾…åŠ©å‡½æ•° ---
def is_non_english_text(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«éè‹±è¯­å­—ç¬¦ï¼ˆå¦‚æ³¢æ–¯è¯­ã€é˜¿æ‹‰ä¼¯è¯­ç­‰ç‰¹æ®Šå­—ç¬¦ï¼‰"""
    if not isinstance(text, str) or not text.strip():
        return False
    
    # å®šä¹‰éæ‹‰ä¸å­—ç¬¦èŒƒå›´ï¼Œä½†æ’é™¤å¸¸è§çš„å›½å®¶åç§°å’Œä»£ç å¯èƒ½ä½¿ç”¨çš„å­—ç¬¦
    # æˆ‘ä»¬éœ€è¦æ›´ç²¾ç¡®åœ°è¯†åˆ«çœŸæ­£éœ€è¦è¿‡æ»¤çš„å­—ç¬¦
    problematic_char_ranges = [
        ('\u0600', '\u06FF'),  # é˜¿æ‹‰ä¼¯è¯­åŠæ³¢æ–¯è¯­
        ('\u0750', '\u077F'),  # é˜¿æ‹‰ä¼¯æ–‡è¡¥å……
        ('\u08A0', '\u08FF'),  # é˜¿æ‹‰ä¼¯æ–‡æ‰©å±•-A
    ]
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜å­—ç¬¦
    for char in text:
        # åªæ£€æŸ¥çœŸæ­£å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—ç¬¦èŒƒå›´
        for start, end in problematic_char_ranges:
            if start <= char <= end:
                return True
    
    # åªè¿‡æ»¤é›¶å®½è¿æ¥ç¬¦ç­‰çœŸæ­£çš„é—®é¢˜å­—ç¬¦
    problematic_chars = ['\u200C', '\u200D']  # é›¶å®½è¿æ¥ç¬¦
    for char in text:
        if char in problematic_chars:
            return True
    
    # ä¿ç•™å¸¸è§çš„å›½å®¶åç§°å­—ç¬¦ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ç­‰
    # è¿™äº›å­—ç¬¦å¯¹äºå›½å®¶è¯†åˆ«å¾ˆé‡è¦ï¼Œä¸åº”è¯¥è¢«è¿‡æ»¤
    return False

# --- Base64 Decoding Helper ---
def decode_base64(data):
    """å®‰å…¨åœ°è§£ç Base64å­—ç¬¦ä¸²ï¼Œå¤„ç†URLå®‰å…¨çš„Base64æ ¼å¼"""
    if not data or not isinstance(data, str):
        return None
    try:
        # æ›¿æ¢URLå®‰å…¨çš„Base64å­—ç¬¦
        data = data.replace('_', '/').replace('-', '+')
        # æ·»åŠ å¿…è¦çš„å¡«å……
        missing_padding = len(data) % 4
        if missing_padding:
            data += '=' * (4 - missing_padding)
        return base64.b64decode(data).decode('utf-8')
    except Exception:
        return None

# --- åè®®åç§°æå–è¾…åŠ©å‡½æ•° ---
def get_vmess_name(vmess_config):
    """
    ä»VMessé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        vmess_config: VMessé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(vmess_config, str) or not vmess_config.startswith('vmess://'):
            return None
        
        # ç§»é™¤å‰ç¼€
        encoded_part = vmess_config[8:]
        
        # å°è¯•è§£ç 
        try:
            # æ·»åŠ å¿…è¦çš„å¡«å……
            padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
            decoded = base64.b64decode(padded).decode('utf-8')
        except Exception:
            # å¦‚æœæ ‡å‡†è§£ç å¤±è´¥ï¼Œå°è¯•URLè§£ç åå†base64è§£ç 
            try:
                encoded_part = unquote(encoded_part)
                padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
                decoded = base64.b64decode(padded).decode('utf-8')
            except Exception:
                return None
        
        # è§£æJSONå¹¶å°è¯•è·å–åç§°
        try:
            vmess_data = json.loads(decoded)
            # å°è¯•ä»ä¸åŒå­—æ®µè·å–åç§°
            for name_field in ['ps', 'name', 'remarks', 'tag']:
                if name_field in vmess_data and isinstance(vmess_data[name_field], str):
                    return vmess_data[name_field].strip()
        except Exception:
            return None
        
        return None
    except Exception:
        return None

def get_ssr_name(ssr_config):
    """
    ä»SSRé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        ssr_config: SSRé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(ssr_config, str) or not ssr_config.startswith('ssr://'):
            return None
        
        # ç§»é™¤å‰ç¼€
        encoded_part = ssr_config[6:]
        
        # å°è¯•è§£ç 
        try:
            # æ·»åŠ å¿…è¦çš„å¡«å……
            padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
            decoded = base64.b64decode(padded).decode('utf-8')
        except Exception:
            # å¦‚æœæ ‡å‡†è§£ç å¤±è´¥ï¼Œå°è¯•URLè§£ç åå†base64è§£ç 
            try:
                encoded_part = unquote(encoded_part)
                padded = encoded_part + '=' * ((4 - len(encoded_part) % 4) % 4)
                decoded = base64.b64decode(padded).decode('utf-8')
            except Exception:
                return None
        
        # SSRæ ¼å¼: server:port:protocol:method:obfs:password_base64/?params
        parts = decoded.split('/?')
        if len(parts) < 2:
            return None
            
        # è§£æå‚æ•°éƒ¨åˆ†å¹¶è·å–remarks
        params = parse_qs(parts[1])
        if 'remarks' in params:
            try:
                remarks_encoded = params['remarks'][0]
                # è§£ç remarks
                padded_remarks = remarks_encoded + '=' * ((4 - len(remarks_encoded) % 4) % 4)
                return base64.b64decode(padded_remarks).decode('utf-8', errors='ignore')
            except Exception:
                return None
        
        return None
    except Exception:
        return None

def get_trojan_name(trojan_config):
    """
    ä»Trojané…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        trojan_config: Trojané…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(trojan_config, str) or not trojan_config.startswith('trojan://'):
            return None
        
        # Trojan URL æ ¼å¼: trojan://password@hostname:port#name
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in trojan_config:
            try:
                name_part = trojan_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLè·¯å¾„æˆ–æŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = trojan_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_vless_name(vless_config):
    """
    ä»VLESSé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        vless_config: VLESSé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(vless_config, str) or not vless_config.startswith('vless://'):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in vless_config:
            try:
                name_part = vless_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = vless_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_shadowsocks_name(ss_config):
    """
    ä»Shadowsocksé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        ss_config: Shadowsocksé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(ss_config, str) or not ss_config.startswith('ss://'):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in ss_config:
            try:
                name_part = ss_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_tuic_name(tuic_config):
    """
    ä»Tuicé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        tuic_config: Tuicé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(tuic_config, str) or not tuic_config.startswith('tuic://'):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in tuic_config:
            try:
                name_part = tuic_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = tuic_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_hysteria2_name(hy2_config):
    """
    ä»Hysteria2é…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        hy2_config: Hysteria2é…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(hy2_config, str) or not (hy2_config.startswith('hy2://') or hy2_config.startswith('hysteria2://')):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in hy2_config:
            try:
                name_part = hy2_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = hy2_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

def get_wireguard_name(wg_config):
    """
    ä»WireGuardé…ç½®ä¸­æå–åç§°ä¿¡æ¯
    å‚æ•°:
        wg_config: WireGuardé…ç½®å­—ç¬¦ä¸²
    è¿”å›:
        æå–çš„åç§°å­—ç¬¦ä¸²æˆ–None
    """
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(wg_config, str) or not wg_config.startswith('wireguard://'):
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ # åçš„åç§°éƒ¨åˆ†
        if '#' in wg_config:
            try:
                name_part = wg_config.split('#', 1)[1]
                return unquote(name_part).strip()
            except Exception:
                pass
        
        # å°è¯•ä»URLæŸ¥è¯¢å‚æ•°ä¸­æå–åç§°
        parts = wg_config.split('?')
        if len(parts) > 1:
            try:
                params = parse_qs(parts[1])
                for name_key in ['name', 'remarks', 'ps']:
                    if name_key in params:
                        return unquote(params[name_key][0]).strip()
            except Exception:
                pass
        
        return None
    except Exception:
        return None

# --- New Filter Function ---
def should_filter_config(config):
    """æ ¹æ®ç‰¹å®šè§„åˆ™è¿‡æ»¤æ— æ•ˆæˆ–ä½è´¨é‡çš„é…ç½®"""
    if not config or not isinstance(config, str):
        return True
    
    # å¿«é€Ÿæ£€æŸ¥ç©ºé…ç½®
    config_stripped = config.strip()
    if not config_stripped:
        return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ»¤çŸ­è¯­
    if FILTERED_PHRASE in config_stripped.lower():
        logging.debug(f"é…ç½®å› åŒ…å«è¿‡æ»¤çŸ­è¯­ '{FILTERED_PHRASE}' è¢«è¿‡æ»¤: {config_stripped[:100]}...")
        return True
    
    # æ£€æŸ¥URLç¼–ç æƒ…å†µ
    percent25_count = config_stripped.count('%25')
    if percent25_count >= MIN_PERCENT25_COUNT * 2:  # æé«˜é˜ˆå€¼ä»¥å‡å°‘è¯¯åˆ¤
        logging.debug(f"é…ç½®å› è¿‡åº¦URLç¼–ç  ({percent25_count}ä¸ª%25) è¢«è¿‡æ»¤: {config_stripped[:100]}...")
        return True
    
    # æ£€æŸ¥é…ç½®é•¿åº¦
    if len(config_stripped) >= MAX_CONFIG_LENGTH * 2:  # æé«˜é˜ˆå€¼ä»¥å‡å°‘è¯¯åˆ¤
        logging.debug(f"é…ç½®å› è¿‡é•¿ ({len(config_stripped)}å­—ç¬¦) è¢«è¿‡æ»¤")
        return True
    
    # ä¼˜åŒ–çš„åè®®å‰ç¼€æ£€æŸ¥
    # ä¼˜åŒ–çš„åè®®å‰ç¼€æ£€æŸ¥ - ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼
    config_lower = config_stripped.lower()
    found_protocol = None
    
    # æ‰å¹³åŒ–PROTOCOL_PREFIXESå­—å…¸ä¸­çš„æ‰€æœ‰å‰ç¼€
    all_protocol_prefixes = [prefix for prefix_list in PROTOCOL_PREFIXES.values() for prefix in prefix_list]
    
    # ä¼˜åŒ–å‰ç¼€åŒ¹é…é€»è¾‘ï¼šå¿…é¡»ä»å­—ç¬¦ä¸²å¼€å¤´åŒ¹é…åè®®å‰ç¼€
    for protocol_prefix in all_protocol_prefixes:
        if config_lower.startswith(protocol_prefix):
            found_protocol = protocol_prefix
            break
    
    if not found_protocol:
        logging.debug(f"é…ç½®å› ç¼ºå°‘æœ‰æ•ˆåè®®å‰ç¼€è¢«è¿‡æ»¤: {config_stripped[:100]}...")
        return True
    
    # å¯¹ä¸åŒåè®®è¿›è¡ŒåŸºæœ¬æ ¼å¼éªŒè¯ - ä½¿ç”¨æ˜ å°„è¡¨ç®€åŒ–é€»è¾‘
    protocol_validation = {
        'vmess://': (8, lambda c: not c[8:].strip()),
        'vless://': (8, lambda c: not c[8:].strip()),
        'trojan://': (0, lambda c: '@' not in c),  # Trojanæ ¼å¼å¿…é¡»åŒ…å«@
        'ss://': (5, lambda c: not c[5:].strip()),
        'ssr://': (6, lambda c: not c[6:].strip()),
        'tuic://': (7, lambda c: not c[7:].strip()),
        'hy2://': (5, lambda c: not c[5:].strip()),
        'hysteria2://': (12, lambda c: not c[12:].strip()),
        'wireguard://': (12, lambda c: not c[12:].strip()),
        'wg://': (5, lambda c: not c[5:].strip())  # æ·»åŠ wireguardçš„wg://å‰ç¼€éªŒè¯
    }
    
    # æ£€æŸ¥åè®®æ˜¯å¦éœ€è¦ç‰¹æ®ŠéªŒè¯ä¸”éªŒè¯å¤±è´¥
    if found_protocol in protocol_validation:
        _, validate_func = protocol_validation[found_protocol]
        if validate_func(config):
            return True
    
    return False

async def fetch_url(session, url, max_retries=2):
    """å¼‚æ­¥è·å–URLå†…å®¹å¹¶æå–æ–‡æœ¬ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
    # éªŒè¯URLæ ¼å¼ï¼ˆæ›´ä¸¥æ ¼çš„éªŒè¯ï¼‰
    if not url or not isinstance(url, str) or not url.startswith(('http://', 'https://')):
        logging.warning(f"æ— æ•ˆçš„URLæ ¼å¼: {url}")
        return url, None
    
    retry_count = 0
    last_exception = None
    
    # ä¼˜åŒ–çš„æµè§ˆå™¨å¤´éƒ¨ï¼Œå¢åŠ æ›´å¤šä¼ªè£…ä¿¡æ¯
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    while retry_count <= max_retries:
        try:
            # æ·»åŠ è¯·æ±‚è¶…æ—¶å’Œé‡å®šå‘å¤„ç†
            timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)
            async with session.get(url, timeout=timeout, headers=headers, allow_redirects=True) as response:
                # å³ä½¿çŠ¶æ€ç ä¸æ˜¯2xxï¼Œä¹Ÿå°è¯•è·å–å†…å®¹
                text_content = None
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæˆåŠŸå“åº”
                if response.status >= 200 and response.status < 300:
                    # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡å¤§çš„å“åº”
                    content_length = response.headers.get('Content-Length')
                    if content_length:
                        try:
                            if int(content_length) > MAX_PAGE_SIZE:
                                logging.warning(f"é¡µé¢è¿‡å¤§ (>{MAX_PAGE_SIZE/1024/1024:.1f}MB), è·³è¿‡: {url}")
                                return url, None
                        except ValueError:
                            pass
                    
                    # å°è¯•å¤„ç†ä¸åŒçš„å†…å®¹ç±»å‹
                    content_type = response.headers.get('Content-Type', '')
                    
                    try:
                        # ä¼˜åŒ–å†…å®¹å¤„ç†é€»è¾‘
                        if 'application/json' in content_type:
                            try:
                                json_data = await response.json()
                                text_content = json.dumps(json_data, ensure_ascii=False)
                                logging.debug(f"å¤„ç†JSONå†…å®¹: {url}")
                            except json.JSONDecodeError:
                                # å›é€€åˆ°æ–‡æœ¬å¤„ç†
                                html = await response.text()
                                soup = BeautifulSoup(html, 'html.parser')
                                text_content = soup.get_text(separator='\n', strip=True)
                        else:
                            # å¤„ç†HTMLæˆ–çº¯æ–‡æœ¬
                            html = await response.text(max_chars=MAX_PAGE_SIZE)  # é™åˆ¶è¯»å–å¤§å°
                            
                            # å†æ¬¡æ£€æŸ¥å†…å®¹å¤§å°
                            if len(html) >= MAX_PAGE_SIZE:
                                logging.warning(f"é¡µé¢å†…å®¹è¿‡å¤§, å·²éƒ¨åˆ†è¯»å–: {url}")
                            
                            # ä¼˜åŒ–BeautifulSoupè§£æ
                            soup = BeautifulSoup(html, 'lxml')  # ä½¿ç”¨lxmlè§£æå™¨æ›´å¿«
                            
                            # ä¼˜åŒ–å†…å®¹æå–ç­–ç•¥
                            # 1. ä¼˜å…ˆä»ä»£ç å—æå–
                            code_blocks = soup.find_all(['pre', 'code'])
                            if code_blocks:
                                text_content = '\n'.join(block.get_text().strip() for block in code_blocks)
                            else:
                                # 2. æå–æ‰€æœ‰æ–‡æœ¬ï¼Œä½†é¿å…é‡å¤
                                text_content = soup.get_text(separator='\n', strip=True)
                                
                    except UnicodeDecodeError:
                        # å¤„ç†ç¼–ç é”™è¯¯
                        logging.warning(f"è§£ç é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼: {url}")
                        content = await response.read()
                        try:
                            text_content = content.decode('utf-8', errors='replace')
                        except:
                            text_content = str(content)[:MAX_PAGE_SIZE]
                else:
                    # éæˆåŠŸå“åº”ä¹Ÿè®°å½•çŠ¶æ€ç 
                    logging.warning(f"URLè¿”å›éæˆåŠŸçŠ¶æ€ç : {response.status}, URL: {url}")
                    # å°è¯•è·å–é”™è¯¯é¡µé¢å†…å®¹
                    try:
                        text_content = await response.text(max_chars=1000)
                        if text_content and len(text_content.strip()) > 0:
                            logging.info(f"æˆåŠŸè·å–: {url}")
                            return url, text_content
                    except Exception:
                        logging.debug(f"æ— æ³•è·å–é”™è¯¯é¡µé¢å†…å®¹: {url}")
                    
        except asyncio.TimeoutError:
            last_exception = "è¯·æ±‚è¶…æ—¶"
            logging.warning(f"è·å–URLè¶…æ—¶: {url}, ç¬¬{retry_count+1}æ¬¡å°è¯•")
        except aiohttp.ClientError as e:
            last_exception = f"å®¢æˆ·ç«¯é”™è¯¯: {str(e)}"
            logging.warning(f"è·å–URLå®¢æˆ·ç«¯é”™è¯¯: {url}, é”™è¯¯: {str(e)}, ç¬¬{retry_count+1}æ¬¡å°è¯•")
        except Exception as e:
            last_exception = f"æœªçŸ¥é”™è¯¯: {type(e).__name__}: {str(e)}"
            logging.warning(f"è·å–URLæ—¶å‡ºé”™: {url}, é”™è¯¯ç±»å‹: {type(e).__name__}, ç¬¬{retry_count+1}æ¬¡å°è¯•")
        
        retry_count += 1
        # åªæœ‰åœ¨è¿˜æ²¡è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°æ—¶æ‰å»¶è¿Ÿ
        if retry_count <= max_retries:
            # æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œå¢åŠ éšæœºå› å­é¿å…é›ªå´©
            delay = min(2 ** retry_count + random.uniform(0, 1), 10)
            logging.info(f"å°†åœ¨{delay:.2f}ç§’åé‡è¯•è·å–URL: {url}")
            await asyncio.sleep(delay)
    
    logging.error(f"åœ¨{max_retries+1}æ¬¡å°è¯•åè·å–URLå¤±è´¥: {url}, æœ€åé”™è¯¯: {last_exception}")
    return url, None

def get_memory_usage():
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
    try:
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        return mem_info.rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
    except Exception as e:
        logging.warning(f"æ— æ³•è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ: {e}")
        return None

# é¢„ç¼–è¯‘åè®®å‰ç¼€çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œæé«˜æ€§èƒ½ - ç¡®ä¿é”®åä¸PROTOCOL_PREFIXESä¸€è‡´
PROTOCOL_REGEX_PATTERNS = {
    'vmess': re.compile(r'vmess://[-_a-zA-Z0-9+/=]+'),
    'vless': re.compile(r'vless://[-_a-zA-Z0-9+/=?&]+'),
    'trojan': re.compile(r'trojan://[-_a-zA-Z0-9+/=?&@.]+'),
    'shadowsocks': re.compile(r'ss://[-_a-zA-Z0-9+/=?&@.]+'),
    'shadowsocksr': re.compile(r'ssr://[-_a-zA-Z0-9+/=?&@.]+'),
    'wireguard': re.compile(r'(wg://|wireguard://)[-_a-zA-Z0-9+/=?&@.\s]+'),
    'tuic': re.compile(r'tuic://[-_a-zA-Z0-9+/=?&@.]+'),
    'hysteria2': re.compile(r'(hy2://|hysteria2://)[-_a-zA-Z0-9+/=?&@.]+')
}

def find_matches(text, categories_data):
    """æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾åŒ¹é…é¡¹ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½"""
    if not text or not isinstance(text, str):
        return {}
    
    matches = {}
    text_lower = text.lower()  # é¢„è®¡ç®—å°å†™æ–‡æœ¬ä»¥æé«˜æ•ˆç‡
    
    # 1. å¿«é€Ÿæ‰«æï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„åè®®æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œåˆæ­¥åŒ¹é…
    # è¿™æ¯”ç”¨æˆ·è‡ªå®šä¹‰çš„æ¨¡å¼æ›´é«˜æ•ˆï¼Œç”¨äºå¿«é€Ÿè¿‡æ»¤å’Œåˆæ­¥æå–
    for protocol, regex in PROTOCOL_REGEX_PATTERNS.items():
        if protocol in categories_data:
            try:
                # å¿«é€Ÿæå–åè®®é“¾æ¥
                quick_matches = regex.findall(text)
                if quick_matches and protocol not in matches:
                    matches[protocol] = set()
                matches[protocol].update(quick_matches)
            except Exception as e:
                logging.debug(f"åè®®å¿«é€ŸåŒ¹é…é”™è¯¯ ({protocol}): {e}")
    
    # 2. åº”ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„æ¨¡å¼è¿›è¡Œæ›´ç²¾ç¡®çš„åŒ¹é…
    for category, patterns in categories_data.items():
        if not patterns or not isinstance(patterns, list):
            continue
            
        category_matches = set()
        is_protocol_category = category in PROTOCOL_CATEGORIES
        
        # å¦‚æœå·²ç»é€šè¿‡å¿«é€ŸåŒ¹é…æ‰¾åˆ°äº†ç»“æœï¼Œåªåœ¨å¿…è¦æ—¶åº”ç”¨è‡ªå®šä¹‰æ¨¡å¼
        if category in matches and len(matches[category]) > 0 and len(patterns) > 3:
            # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿçš„ç»“æœä¸”æ¨¡å¼å¾ˆå¤šï¼Œè·³è¿‡ä»¥æé«˜æ€§èƒ½
            continue
        
        for pattern_str in patterns:
            if not isinstance(pattern_str, str) or not pattern_str.strip():
                continue
                
            try:
                # å¯¹äºå›½å®¶ç±»åˆ«ï¼Œä½¿ç”¨åŸå§‹æ­£åˆ™è¡¨è¾¾å¼
                pattern = re.compile(
                    pattern_str, 
                    re.IGNORECASE | re.MULTILINE | re.DOTALL | re.UNICODE
                )
                found = pattern.findall(text)
                
                # æ‰¹é‡æ·»åŠ åŒ¹é…é¡¹ï¼Œå‡å°‘å¾ªç¯å¼€é”€
                valid_items = {item.strip() for item in found 
                            if item and isinstance(item, str) and item.strip()}
                category_matches.update(valid_items)
                
            except re.error as e:
                logging.error(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯ - æ¨¡å¼ '{pattern_str[:50]}...' åœ¨ç±»åˆ« '{category}': {e}")
                continue
                
        if category_matches:  # åªæ·»åŠ éç©ºé›†åˆ
            if category not in matches:
                matches[category] = set()
            matches[category].update(category_matches)
    
    # 3. æ¸…ç†ï¼šç¡®ä¿æ‰€æœ‰ç»“æœéƒ½æ˜¯æœ‰æ•ˆçš„URLæ ¼å¼
    for category in list(matches.keys()):
        # æ£€æŸ¥æ˜¯å¦æ˜¯åè®®ç±»åˆ«
        category_lower = category.lower()
        for proto, prefixes in PROTOCOL_PREFIXES.items():
            if proto == category_lower:
                valid_configs = {config for config in matches[category] 
                               if any(config.startswith(prefix) for prefix in prefixes)}
                matches[category] = valid_configs
                break
    
    return matches

def save_to_file(directory, category_name, items_set):
    """å°†é¡¹ç›®é›†åˆä¿å­˜åˆ°æŒ‡å®šç›®å½•çš„æ–‡æœ¬æ–‡ä»¶ä¸­"""
    if not items_set:
        logging.debug(f"è·³è¿‡ç©ºé›†åˆçš„ä¿å­˜: {category_name}")
        return False, 0
        
    # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
    abs_directory = os.path.abspath(directory)
    abs_file_path = os.path.join(abs_directory, f"{category_name}.txt")
    count = len(items_set)
    
    # æ·»åŠ æ—¥å¿—ï¼Œè®°å½•å°†ä¿å­˜çš„å†…å®¹æ•°é‡å’Œç›®æ ‡ä½ç½®
    logging.debug(f"å‡†å¤‡ä¿å­˜ {count} é¡¹åˆ°: {abs_file_path}")
    
    # è®°å½•å°†è¦ä¿å­˜çš„æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    logging.info(f"å‡†å¤‡ä¿å­˜ {count} é¡¹åˆ°: {abs_file_path}")
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(abs_directory, exist_ok=True)
        logging.debug(f"ç¡®è®¤ç›®å½•å­˜åœ¨: {abs_directory}")
        
        # ç›´æ¥å†™å…¥æ–‡ä»¶
        with open(abs_file_path, 'w', encoding='utf-8', newline='') as f:
            for item in sorted(list(items_set)):
                f.write(f"{item}\n")
        
        # å¼ºåˆ¶åˆ·æ–°æ–‡ä»¶ç³»ç»Ÿç¼“å­˜
        import io
        io.open(abs_file_path).close()
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸå†™å…¥
        if os.path.exists(abs_file_path):
            file_size = os.path.getsize(abs_file_path)
            if file_size > 0:
                logging.info(f"âœ“ æˆåŠŸä¿å­˜ {count} é¡¹åˆ° {abs_file_path} (å¤§å°: {file_size} å­—èŠ‚)")
                return True, count
            else:
                logging.error(f"âœ— æ–‡ä»¶åˆ›å»ºæˆåŠŸä½†ä¸ºç©º: {abs_file_path}")
                return False, 0
        else:
            logging.error(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {abs_file_path}")
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å¯å†™
            if not os.access(abs_directory, os.W_OK):
                logging.error(f"âœ— ç›®å½•ä¸å¯å†™: {abs_directory}")
            else:
                logging.debug(f"ç›®å½•å¯å†™ï¼Œä½†æ–‡ä»¶åˆ›å»ºå¤±è´¥")
            
            return False, 0
    except Exception as e:
        logging.error(f"âœ— ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        # ä½¿ç”¨å¤‡ç”¨æ–¹æ³• - å†™å…¥åˆ°ä¸´æ—¶æ–‡ä»¶å¹¶ç«‹å³æ£€æŸ¥
        try:
            temp_file = os.path.join(abs_directory, f"temp_{category_name}.txt")
            logging.info(f"å°è¯•å¤‡ç”¨æ–¹æ³•ï¼Œå†™å…¥ä¸´æ—¶æ–‡ä»¶: {temp_file}")
            
            with open(temp_file, 'w', encoding='utf-8') as f:
                # å†™å…¥å°‘é‡å†…å®¹ç”¨äºæµ‹è¯•
                sample_items = list(items_set)[:10]  # å–å‰10ä¸ªæ ·æœ¬
                for item in sorted(sample_items):
                    f.write(f"{item}\n")
            
            if os.path.exists(temp_file):
                logging.info(f"å¤‡ç”¨æ–¹æ³•æµ‹è¯•æˆåŠŸï¼Œä¸´æ—¶æ–‡ä»¶å·²åˆ›å»º")
                # ç°åœ¨å†™å…¥å®Œæ•´å†…å®¹
                with open(temp_file, 'w', encoding='utf-8') as f:
                    for item in sorted(list(items_set)):
                        f.write(f"{item}\n")
                
                # é‡å‘½ååˆ°ç›®æ ‡ä½ç½®
                if os.path.exists(abs_file_path):
                    os.remove(abs_file_path)
                os.rename(temp_file, abs_file_path)
                
                logging.info(f"âœ“ å¤‡ç”¨æ–¹æ³•: å·²ä¿å­˜ {count} é¡¹åˆ° {abs_file_path}")
                return True, count
            else:
                logging.error(f"âœ— å¤‡ç”¨æ–¹æ³•å¤±è´¥: ä¸´æ—¶æ–‡ä»¶æœªåˆ›å»º")
                return False, 0
        except Exception as backup_e:
            logging.error(f"âœ— å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {str(backup_e)}")
            return False, 0

# --- ä½¿ç”¨æ——å¸œå›¾åƒç”Ÿæˆç®€å•çš„READMEå‡½æ•° ---
def generate_simple_readme(protocol_counts, country_counts, all_keywords_data, use_local_paths=True):
    """ç”Ÿæˆå¢å¼ºç‰ˆREADME.mdæ–‡ä»¶ï¼Œå±•ç¤ºæŠ“å–ç»“æœç»Ÿè®¡ä¿¡æ¯"""
    # è¾“å…¥éªŒè¯å’Œé»˜è®¤å€¼å¤„ç†
    protocol_counts = protocol_counts if isinstance(protocol_counts, dict) else {}
    country_counts = country_counts if isinstance(country_counts, dict) else {}
    all_keywords_data = all_keywords_data if isinstance(all_keywords_data, dict) else {}
    
    try:
        # è·å–å¸¦æ—¶åŒºçš„å½“å‰æ—¶é—´
        tz = pytz.timezone('Asia/Shanghai')
        now = datetime.now(tz)
        timestamp = now.strftime("%Y-%m-%d %H:%M:%S %Z")
        date_only = now.strftime("%Y-%m-%d")
        
        # è®¡ç®—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        total_protocol_configs = sum(protocol_counts.values())
        total_country_configs = sum(country_counts.values())
        countries_with_data = len(country_counts)
        protocols_with_data = len(protocol_counts)
        
        # è®¡ç®—å¹³å‡æ¯å›½é…ç½®æ•°
        avg_configs_per_country = total_country_configs / countries_with_data if countries_with_data > 0 else 0
        
        # æ‰¾å‡ºé…ç½®æœ€å¤šçš„å›½å®¶å’Œåè®®
        top_country = max(country_counts.items(), key=lambda x: x[1], default=("æ— ", 0))
        top_protocol = max(protocol_counts.items(), key=lambda x: x[1], default=("æ— ", 0))

        # æ„å»ºå­ç›®å½•çš„è·¯å¾„
        if use_local_paths:
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä¾¿äºæœ¬åœ°æŸ¥çœ‹
            protocol_base_url = f"{PROTOCOL_SUBDIR}"
            country_base_url = f"{COUNTRY_SUBDIR}"
            logging.debug(f"READMEä½¿ç”¨æœ¬åœ°ç›¸å¯¹è·¯å¾„: protocols={protocol_base_url}, countries={country_base_url}")
        else:
            # GitHubè¿œç¨‹è·¯å¾„æ”¯æŒ
            github_repo_path = "miladtahanian/V2RayScrapeByCountry"
            github_branch = "main"
            protocol_base_url = f"https://raw.githubusercontent.com/{github_repo_path}/refs/heads/{github_branch}/{OUTPUT_DIR}/{PROTOCOL_SUBDIR}"
            country_base_url = f"https://raw.githubusercontent.com/{github_repo_path}/refs/heads/{github_branch}/{OUTPUT_DIR}/{COUNTRY_SUBDIR}"
            logging.debug(f"READMEä½¿ç”¨GitHubè·¯å¾„: protocols={protocol_base_url}, countries={country_base_url}")

        # æ„å»ºREADMEå†…å®¹ï¼Œä½¿ç”¨æ›´ç°ä»£åŒ–çš„æ ¼å¼
        md_content = [
            f"# ğŸ“Š V2Ray é…ç½®æŠ“å–ç»“æœ ({date_only})\n",
            "\n",
            f"*æœ€åæ›´æ–°: {timestamp}*\n",
            "\n",
            "> æ­¤æ–‡ä»¶ç”±è‡ªåŠ¨è„šæœ¬ç”Ÿæˆï¼ŒåŒ…å«ä»å¤šä¸ªæ¥æºæŠ“å–å’Œåˆ†ç±»çš„ V2Ray é…ç½®ä¿¡æ¯ã€‚\n",
            "\n",
            "## ğŸ“‹ è¯¦ç»†ç»Ÿè®¡æ¦‚è§ˆ\n",
            "\n",
            f"- **æ€»é…ç½®æ•°é‡**: **{total_protocol_configs:,}**\n",
            f"- **æœ‰æ•°æ®çš„åè®®ç±»å‹**: {protocols_with_data}\n",
            f"- **å›½å®¶ç›¸å…³é…ç½®æ•°**: {total_country_configs:,}\n",
            f"- **æœ‰é…ç½®çš„å›½å®¶/åœ°åŒº**: {countries_with_data}\n",
            f"- **å¹³å‡æ¯å›½é…ç½®æ•°**: {avg_configs_per_country:.1f}\n",
            f"- **é…ç½®æœ€å¤šçš„å›½å®¶**: {top_country[0]} ({top_country[1]:,} ä¸ªé…ç½®)\n",
            f"- **é…ç½®æœ€å¤šçš„åè®®**: {top_protocol[0]} ({top_protocol[1]:,} ä¸ªé…ç½®)\n",
            "\n",
            "## â„¹ï¸ è¯´æ˜\n",
            "\n",
            "- å›½å®¶æ–‡ä»¶ä»…åŒ…å«åœ¨**é…ç½®åç§°**ä¸­æ‰¾åˆ°å›½å®¶åç§°/æ ‡è¯†çš„é…ç½®\n",
            "- é…ç½®åç§°é¦–å…ˆä»é“¾æ¥çš„`#`éƒ¨åˆ†æå–ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä»å†…éƒ¨åç§°(å¯¹äºVmess/SSR)æå–\n",
            "- æ‰€æœ‰é…ç½®å·²æŒ‰ç±»åˆ«æ•´ç†åˆ°ä¸åŒç›®å½•ä¸­ï¼Œä¾¿äºæŸ¥æ‰¾å’Œä½¿ç”¨\n",
            "- é…ç½®å¯èƒ½éšæ—¶å¤±æ•ˆï¼Œè¯·åŠæ—¶æ›´æ–°\n",
            "\n",
        ]

        # æ·»åŠ åè®®æ–‡ä»¶è¡¨æ ¼
        md_content.append("## ğŸ“ åè®®é…ç½®æ–‡ä»¶\n")
        md_content.append("\n")
        
        if protocol_counts:
            # æŒ‰é…ç½®æ•°é‡æ’åº
            sorted_protocols = sorted(protocol_counts.items(), key=lambda x: x[1], reverse=True)
            
            md_content.append("| åè®®ç±»å‹ | é…ç½®æ•°é‡ | å æ¯” | æ–‡ä»¶é“¾æ¥ |")
            md_content.append("|---------|---------|------|----------|")
            
            for category_name, count in sorted_protocols:
                # è®¡ç®—å æ¯”
                percentage = (count / total_protocol_configs * 100) if total_protocol_configs > 0 else 0
                file_link = f"{protocol_base_url}/{category_name}.txt"
                md_content.append(f"| **{category_name}** | {count:,} | {percentage:.1f}% | [`{category_name}.txt`]({file_link}) |")
        else:
            md_content.append("*æ²¡æœ‰æ‰¾åˆ°åè®®é…ç½®ã€‚*\n")
        
        md_content.append("\n")

        # æ·»åŠ å›½å®¶æ–‡ä»¶è¡¨æ ¼
        md_content.append("## ğŸŒ å›½å®¶/åœ°åŒºé…ç½®æ–‡ä»¶\n")
        md_content.append("\n")
        
        if country_counts:
            # æŒ‰é…ç½®æ•°é‡æ’åº
            sorted_countries = sorted(country_counts.items(), key=lambda x: x[1], reverse=True)
            
            md_content.append("| å›½å®¶/åœ°åŒº | é…ç½®æ•°é‡ | æ–‡ä»¶é“¾æ¥ |")
            md_content.append("|----------|---------|----------|")
            
            for country_category_name, count in sorted_countries:
                country_display_text = []
                
                # 1. æŸ¥æ‰¾å›½å®¶çš„ä¸¤å­—æ¯ISOä»£ç ç”¨äºæ——å¸œå›¾åƒ
                flag_image_markdown = ""
                if country_category_name in all_keywords_data:
                    keywords_list = all_keywords_data[country_category_name]
                    if isinstance(keywords_list, list):
                        for item in keywords_list:
                            if isinstance(item, str) and len(item) == 2 and item.isupper() and item.isalpha():
                                iso_code = item.lower()
                                flag_image_url = f"https://flagcdn.com/w20/{iso_code}.png"
                                flag_image_markdown = f'<img src="{flag_image_url}" width="20" height="15" alt="{country_category_name} flag" align="absmiddle">'
                                country_display_text.append(flag_image_markdown)
                                break
                
                # 2. æå–ä¸­æ–‡åç§°ï¼ˆå¦‚æœæœ‰ï¼‰
                display_name = country_category_name
                if country_category_name in all_keywords_data:
                    keywords_list = all_keywords_data[country_category_name]
                    if isinstance(keywords_list, list):
                        # æŸ¥æ‰¾åŒ…å«ä¸­æ–‡çš„æ¡ç›®
                        for item in keywords_list:
                            if isinstance(item, str):
                                # æå–çº¯ä¸­æ–‡éƒ¨åˆ†
                                chinese_chars = ''.join(char for char in item if '\u4e00' <= char <= '\u9fff')
                                if chinese_chars:
                                    display_name = f"{country_category_name}ï¼ˆ{chinese_chars}ï¼‰"
                                    break
                
                country_display_text.append(display_name)
                full_display = " ".join(country_display_text)
                
                # æ„å»ºæ–‡ä»¶é“¾æ¥
                file_link = f"{country_base_url}/{country_category_name}.txt"
                md_content.append(f"| {full_display} | {count:,} | [`{country_category_name}.txt`]({file_link}) |")
        else:
            md_content.append("*æ²¡æœ‰æ‰¾åˆ°ä¸å›½å®¶ç›¸å…³çš„é…ç½®ã€‚*\n")
        
        md_content.append("\n")
        
        # æ·»åŠ åº•éƒ¨ä¿¡æ¯
        md_content.append("## ğŸ“ å¤‡æ³¨\n")
        md_content.append("\n")
        md_content.append("- æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨\n")
        md_content.append("- è¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„\n")
        md_content.append("- å®šæœŸæ›´æ–°ä»¥è·å–æœ€æ–°é…ç½®\n")
        
        # åˆå¹¶å†…å®¹
        full_content = ''.join(md_content)
        
        # ç¡®ä¿READMEæ–‡ä»¶ç›®å½•å­˜åœ¨
        readme_dir = os.path.dirname(README_FILE)
        if readme_dir and not os.path.exists(readme_dir):
            try:
                os.makedirs(readme_dir, exist_ok=True)
                logging.debug(f"åˆ›å»ºREADMEç›®å½•: {readme_dir}")
            except Exception as e:
                logging.error(f"åˆ›å»ºREADMEç›®å½•å¤±è´¥: {e}")
                return False
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(README_FILE, 'w', encoding='utf-8', newline='') as f:
                f.write(full_content)
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸå†™å…¥
            if os.path.exists(README_FILE):
                file_size = os.path.getsize(README_FILE) / 1024  # KB
                logging.info(f"âœ… æˆåŠŸç”ŸæˆREADMEæ–‡ä»¶: {os.path.abspath(README_FILE)} ({file_size:.2f} KB)")
                return True
            else:
                logging.error(f"âŒ READMEæ–‡ä»¶å†™å…¥å¤±è´¥: æ–‡ä»¶ä¸å­˜åœ¨")
                return False
                
        except IOError as e:
            logging.error(f"âŒ å†™å…¥READMEæ–‡ä»¶æ—¶å‘ç”ŸIOé”™è¯¯: {e}")
            return False
        except Exception as e:
            logging.error(f"âŒ ç”ŸæˆREADMEæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return False
            
    except Exception as e:
        logging.error(f"âŒ READMEç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        return False

# mainå‡½æ•°å’Œå…¶ä»–å‡½æ•°å®ç°
async def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªæŠ“å–å’Œå¤„ç†æµç¨‹"""
    start_time = time.time()
    logging.info(f"æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {os.path.abspath(log_file_path)}")
    logging.info(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # ç¡®ä¿é…ç½®æ–‡ä»¶å¤¹å­˜åœ¨
    try:
        os.makedirs(CONFIG_DIR, exist_ok=True)
        logging.info(f"é…ç½®æ–‡ä»¶å¤¹: {os.path.abspath(CONFIG_DIR)}")
    except Exception as e:
        logging.error(f"åˆ›å»ºé…ç½®æ–‡ä»¶å¤¹å¤±è´¥: {e}")
    
    # æ£€æŸ¥å¿…è¦çš„è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    urls_file_abs = os.path.abspath(URLS_FILE)
    keywords_file_abs = os.path.abspath(KEYWORDS_FILE)
    
    if not os.path.exists(urls_file_abs) or not os.path.exists(keywords_file_abs):
        missing_files = []
        if not os.path.exists(urls_file_abs):
            missing_files.append(f"URLsæ–‡ä»¶: {urls_file_abs}")
        if not os.path.exists(keywords_file_abs):
            missing_files.append(f"å…³é”®è¯æ–‡ä»¶: {keywords_file_abs}")
        
        logging.critical(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶:\n- {chr(10)}- ".join(missing_files))
        logging.info(f"è¯·ç¡®ä¿è¿™äº›æ–‡ä»¶å·²æ”¾åœ¨æ­£ç¡®çš„ä½ç½®")
        return
    
    # æ£€æŸ¥æ–‡ä»¶è¯»å–æƒé™
    if not os.access(urls_file_abs, os.R_OK):
        logging.critical(f"æ²¡æœ‰æƒé™è¯»å–URLsæ–‡ä»¶: {urls_file_abs}")
        return
    if not os.access(keywords_file_abs, os.R_OK):
        logging.critical(f"æ²¡æœ‰æƒé™è¯»å–å…³é”®è¯æ–‡ä»¶: {keywords_file_abs}")
        return

    # åŠ è½½URLå’Œå…³é”®è¯æ•°æ®
    try:
        # æ›´å¥å£®çš„URLåŠ è½½ï¼Œè·³è¿‡æ³¨é‡Šè¡Œå’Œæ— æ•ˆURL
        urls = []
        with open(urls_file_abs, 'r', encoding='utf-8', errors='replace') as f:
            for line_num, line in enumerate(f, 1):
                stripped_line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                if stripped_line and not stripped_line.startswith('#'):
                    # åŸºæœ¬URLæ ¼å¼éªŒè¯
                    if stripped_line.startswith(('http://', 'https://')):
                        urls.append(stripped_line)
                    else:
                        logging.warning(f"ç¬¬{line_num}è¡ŒåŒ…å«æ— æ•ˆURLæ ¼å¼: {stripped_line}")
            
        if not urls:
            logging.critical("URLsæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URLã€‚")
            return
            
        logging.info(f"å·²ä» {URLS_FILE} åŠ è½½ {len(urls)} ä¸ªæœ‰æ•ˆURL")
        
        # æ›´å®‰å…¨çš„JSONåŠ è½½
        categories_data = {}
        with open(keywords_file_abs, 'r', encoding='utf-8', errors='replace') as f:
            try:
                categories_data = json.load(f)
            except json.JSONDecodeError as e:
                logging.critical(f"è§£ækeywords.jsonæ–‡ä»¶å¤±è´¥: {e}")
                # å°è¯•æä¾›æ›´å¤šå¸®åŠ©ä¿¡æ¯
                logging.info("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSON")
                return
            
        # éªŒè¯categories_dataæ˜¯å­—å…¸ç±»å‹
        if not isinstance(categories_data, dict):
            logging.critical("keywords.jsonå¿…é¡»åŒ…å«å­—å…¸æ ¼å¼çš„æ•°æ®ã€‚")
            return
            
        # éªŒè¯åè®®ç±»åˆ«æ˜¯å¦åœ¨é…ç½®ä¸­
        missing_protocols = [p for p in PROTOCOL_CATEGORIES if p not in categories_data]
        if missing_protocols:
            logging.warning(f"keywords.jsonä¸­ç¼ºå°‘ä»¥ä¸‹åè®®ç±»åˆ«çš„é…ç½®: {', '.join(missing_protocols)}")
            # ä¸ºç¼ºå¤±çš„åè®®ç±»åˆ«åˆ›å»ºç©ºåˆ—è¡¨ï¼Œä»¥ä¾¿ç¨‹åºèƒ½å¤Ÿç»§ç»­è¿è¡Œ
            for protocol in missing_protocols:
                categories_data[protocol] = []
            
        # éªŒè¯æ¯ä¸ªå€¼éƒ½æ˜¯åˆ—è¡¨
        invalid_entries = []
        valid_categories_data = {}
        for k, v in categories_data.items():
            if isinstance(v, list):
                # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å’Œéå­—ç¬¦ä¸²å…ƒç´ 
                filtered_list = [item for item in v if isinstance(item, str) and item.strip()]
                if filtered_list:  # åªä¿ç•™éç©ºåˆ—è¡¨
                    valid_categories_data[k] = filtered_list
            else:
                invalid_entries.append((k, type(v).__name__))
        
        if invalid_entries:
            logging.warning(f"keywords.jsonåŒ…å«éåˆ—è¡¨æ ¼å¼çš„å€¼: {invalid_entries}")
            
        categories_data = valid_categories_data
        
        if not categories_data:
            logging.critical("keywords.jsonä¸­æ²¡æœ‰æœ‰æ•ˆçš„ç±»åˆ«æ•°æ®ã€‚")
            return
            
        # ç»Ÿè®¡æœ‰æ•ˆå…³é”®è¯ä¿¡æ¯
        total_patterns = sum(len(patterns) for patterns in categories_data.values())
        logging.info(f"æˆåŠŸåŠ è½½ {len(categories_data)} ä¸ªç±»åˆ«ï¼Œå…± {total_patterns} ä¸ªæ¨¡å¼")
            
    except IOError as e:
        logging.critical(f"è¯»å–è¾“å…¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return
    except Exception as e:
        logging.critical(f"åŠ è½½é…ç½®æ•°æ®æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        return

    # åˆ†ç¦»åè®®æ¨¡å¼å’Œå›½å®¶å…³é”®è¯
    protocol_patterns_for_matching = {
        cat: patterns for cat, patterns in categories_data.items() if cat in PROTOCOL_CATEGORIES
    }
    country_keywords_for_naming = {
        cat: patterns for cat, patterns in categories_data.items() if cat not in PROTOCOL_CATEGORIES
    }
    country_category_names = list(country_keywords_for_naming.keys())

    logging.info(f"å·²åŠ è½½ {len(urls)} ä¸ªURLå’Œ {len(categories_data)} ä¸ªæ€»ç±»åˆ«")
    logging.info(f"åè®®ç±»åˆ«æ•°é‡: {len(protocol_patterns_for_matching)}")
    logging.info(f"å›½å®¶ç±»åˆ«æ•°é‡: {len(country_keywords_for_naming)}")

    # URLå»é‡ï¼ˆä½¿ç”¨æœ‰åºå­—å…¸ä¿ç•™é¡ºåºï¼‰
    unique_urls = list(dict.fromkeys(urls))  # Python 3.7+ ä¸­å­—å…¸ä¿æŒæ’å…¥é¡ºåº
    if len(unique_urls) < len(urls):
        duplicate_count = len(urls) - len(unique_urls)
        logging.info(f"å·²å»é™¤ {duplicate_count} ä¸ªé‡å¤URLï¼Œå‰©ä½™ {len(unique_urls)} ä¸ªå”¯ä¸€URL")
        urls = unique_urls
    
    # å¼‚æ­¥è·å–æ‰€æœ‰é¡µé¢
    # åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°ï¼Œæ ¹æ®URLæ•°é‡
    dynamic_concurrency = min(CONCURRENT_REQUESTS, max(5, len(urls) // 10))
    sem = asyncio.Semaphore(dynamic_concurrency)
    
    async def fetch_with_semaphore(session, url_to_fetch):
        """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘çš„fetch_urlï¼Œæ·»åŠ è¶…æ—¶å’Œé‡è¯•æ§åˆ¶"""
        async with sem:
            try:
                # ä¸ºæ¯ä¸ªURLè®°å½•å¼€å§‹æ—¶é—´
                start_fetch_time = time.time()
                result = await fetch_url(session, url_to_fetch)
                fetch_time = time.time() - start_fetch_time
                logging.debug(f"URLè·å–å®Œæˆ: {url_to_fetch[:50]}..., è€—æ—¶: {fetch_time:.2f}ç§’")
                return result
            except asyncio.CancelledError:
                # å¤„ç†ä»»åŠ¡å–æ¶ˆ
                logging.warning(f"URLè·å–ä»»åŠ¡è¢«å–æ¶ˆ: {url_to_fetch}")
                return url_to_fetch, None
            except Exception as e:
                logging.error(f"URLè·å–ä»»åŠ¡å¼‚å¸¸: {url_to_fetch[:50]}..., é”™è¯¯ç±»å‹: {type(e).__name__}")
                return url_to_fetch, None
    
    # åˆ›å»ºä¼˜åŒ–çš„HTTPä¼šè¯
    timeout = aiohttp.ClientTimeout(
        total=60,      # æ€»è¶…æ—¶
        connect=15,    # è¿æ¥è¶…æ—¶
        sock_connect=15,  # socketè¿æ¥è¶…æ—¶
        sock_read=30   # socketè¯»å–è¶…æ—¶
    )
    
    connector = aiohttp.TCPConnector(
        limit=dynamic_concurrency,  # æœ€å¤§å¹¶å‘è¿æ¥æ•°
        limit_per_host=min(5, dynamic_concurrency // 2),  # æ¯ä¸ªä¸»æœºçš„æœ€å¤§è¿æ¥æ•°
        enable_cleanup_closed=True,  # å¯ç”¨è¿æ¥æ¸…ç†
        ttl_dns_cache=300  # DNSç¼“å­˜æ—¶é—´
    )
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        logging.info(f"å¼€å§‹è·å– {len(urls)} ä¸ªURLs (æœ€å¤§å¹¶å‘: {dynamic_concurrency})...")
        
        # æ‰¹é‡å¤„ç†URLï¼Œæ ¹æ®URLæ•°é‡åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
        batch_size = min(20, max(5, len(urls) // 5))
        filtered_pages = []
        success_count = 0
        exception_count = 0
        
        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i+batch_size]
            batch_num = i//batch_size + 1
            total_batches = (len(urls) + batch_size - 1) // batch_size
            logging.info(f"å¤„ç†URLæ‰¹æ¬¡ {batch_num}/{total_batches}, åŒ…å« {len(batch_urls)} ä¸ªURL")
            
            # å¼‚æ­¥è·å–æœ¬æ‰¹æ¬¡URLçš„å†…å®¹
            batch_results = await asyncio.gather(
                *[fetch_with_semaphore(session, u) for u in batch_urls],
                return_exceptions=True  # å³ä½¿æŸäº›ä»»åŠ¡å¤±è´¥ä¹Ÿç»§ç»­æ‰§è¡Œ
            )
            
            # å¤„ç†æœ¬æ‰¹æ¬¡ç»“æœ
            for j, result in enumerate(batch_results):
                url = batch_urls[j]
                if isinstance(result, tuple) and len(result) == 2 and isinstance(result[0], str) and result[1] is not None:
                    filtered_pages.append(result)
                    success_count += 1
                    logging.debug(f"æˆåŠŸè·å–URL: {url}")
                elif isinstance(result, Exception):
                    exception_count += 1
                    logging.warning(f"URLè·å–ä»»åŠ¡å¼‚å¸¸: {url}, {type(result).__name__}: {result}")
                else:
                    logging.debug(f"æ— æ•ˆçš„URLè·å–ç»“æœ: {url}, {type(result)}")
            
            # è®°å½•æ‰¹æ¬¡è¿›åº¦
            logging.info(f"æ‰¹æ¬¡ {batch_num}/{total_batches} å®Œæˆ: æˆåŠŸ {success_count}, å¼‚å¸¸ {exception_count}, ç´¯è®¡æœ‰æ•ˆé¡µé¢ {len(filtered_pages)}")
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œåœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ å°å»¶è¿Ÿ
            if i + batch_size < len(urls):
                logging.debug(f"åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ 1ç§’å»¶è¿Ÿ")
                await asyncio.sleep(1)
        
        fetched_pages = filtered_pages
        logging.info(f"URLè·å–å®Œæˆ: æˆåŠŸ {success_count}, å¼‚å¸¸ {exception_count}, æ€»è®¡ {len(filtered_pages)} ä¸ªé¡µé¢å¾…å¤„ç†")

    # åˆå§‹åŒ–ç»“æœé›†åˆ
    final_configs_by_country = {cat: set() for cat in country_category_names}
    final_all_protocols = {cat: set() for cat in PROTOCOL_CATEGORIES}
    # å…¨å±€å»é‡é›†åˆï¼Œæé«˜å»é‡æ•ˆç‡
    global_config_set = set()

    logging.info("å¤„ç†é¡µé¢å¹¶å…³è”é…ç½®åç§°...")
    
    # ç»Ÿè®¡æˆåŠŸå¤„ç†çš„é¡µé¢æ•°é‡
    processed_pages = 0
    found_configs = 0
    filtered_out_configs = 0
    
    # å¤„ç†å‰è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
    initial_memory = get_memory_usage()
    if initial_memory:
        logging.debug(f"åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.2f} MB")
    
    for page_idx, (url, text) in enumerate(fetched_pages, 1):
        if not text:
            continue
            
        processed_pages += 1
        page_start_time = time.time()
        
        # ä¼˜åŒ–çš„åŒ¹é…å¤„ç†
        try:
            page_protocol_matches = find_matches(text, protocol_patterns_for_matching)
        except Exception as e:
            logging.error(f"å¤„ç†é¡µé¢æ—¶å‡ºé”™: {url}, é”™è¯¯: {e}")
            continue
        
        # å¤„ç†æ‰¾åˆ°çš„åè®®é…ç½®
        page_found_count = 0
        page_filtered_count = 0
        
        # æ£€æŸ¥æ€»é…ç½®æ•°æ˜¯å¦è¶…è¿‡é™åˆ¶ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
        total_current_configs = sum(len(configs) for configs in final_all_protocols.values())
        if total_current_configs >= MAX_TOTAL_CONFIGS:
            logging.warning(f"å·²è¾¾åˆ°æœ€å¤§é…ç½®æ•°é™åˆ¶ ({MAX_TOTAL_CONFIGS})ï¼Œåœæ­¢å¤„ç†æ–°é…ç½®")
            break
        
        # æ‰¹é‡å¤„ç†é…ç½®ï¼Œå‡å°‘é‡å¤è®¡ç®—
        for protocol_cat_name, configs_found in page_protocol_matches.items():
            if protocol_cat_name in PROTOCOL_CATEGORIES:
                # å¯¹æ¯ä¸ªåè®®ç±»åˆ«çš„é…ç½®è¿›è¡Œæ‰¹é‡å¤„ç†
                valid_configs = []
                for config in configs_found:
                    # å¿«é€Ÿæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨äºå…¨å±€é›†åˆä¸­
                    if config in global_config_set:
                        continue
                        
                    # è¿‡æ»¤æ— æ•ˆé…ç½®
                    if not should_filter_config(config):
                        valid_configs.append(config)
                        global_config_set.add(config)
                        page_found_count += 1
                    else:
                        page_filtered_count += 1
                
                # æ‰¹é‡æ·»åŠ åˆ°ç»“æœé›†åˆ
                if valid_configs:
                    final_all_protocols[protocol_cat_name].update(valid_configs)
        
        found_configs += page_found_count
        filtered_out_configs += page_filtered_count
        
        page_processing_time = time.time() - page_start_time
        
        # æ›´æ™ºèƒ½çš„è¿›åº¦è¾“å‡º
        if processed_pages % 10 == 0 or page_idx == len(fetched_pages):
            current_memory = get_memory_usage()
            memory_info = f"å†…å­˜: {current_memory:.2f} MB" if current_memory else ""
            logging.info(f"å¤„ç†è¿›åº¦: {processed_pages}/{len(fetched_pages)} é¡µé¢, "
                      f"å·²æ‰¾åˆ° {found_configs} é…ç½®, å·²è¿‡æ»¤ {filtered_out_configs} é…ç½®, "
                      f"æ­¤é¡µè€—æ—¶: {page_processing_time:.2f}ç§’ {memory_info}")
        elif page_processing_time > 5:  # è®°å½•å¤„ç†æ…¢çš„é¡µé¢
            logging.warning(f"é¡µé¢å¤„ç†è¾ƒæ…¢: {url[:50]}..., è€—æ—¶: {page_processing_time:.2f}ç§’")

        # é…ç½®å·²é€šè¿‡global_config_setè¿›è¡Œå»é‡ï¼Œæ— éœ€é¢å¤–æ“ä½œ
        if len(unique_configs) < len(all_page_configs_after_filter):
            logging.info(f"å»é‡å‰é…ç½®æ•°é‡: {len(all_page_configs_after_filter)}, å»é‡å: {len(unique_configs)}")
            all_page_configs_after_filter = unique_configs
        
        # ä¸ºæ¯ä¸ªé…ç½®å…³è”å›½å®¶ä¿¡æ¯
        for config in all_page_configs_after_filter:
            name_to_check = None
            
            # 1. é¦–å…ˆå°è¯•ä»URLç‰‡æ®µä¸­æå–åç§°ï¼ˆ#åé¢çš„éƒ¨åˆ†ï¼‰
            if '#' in config:
                try:
                    potential_name = config.split('#', 1)[1]
                    name_to_check = unquote(potential_name).strip()
                    if not name_to_check:
                        name_to_check = None
                except (IndexError, Exception) as e:
                    logging.debug(f"ä»URLç‰‡æ®µæå–åç§°å¤±è´¥: {e}")

            # 2. å¦‚æœURLç‰‡æ®µä¸­æ²¡æœ‰åç§°ï¼Œå°è¯•ä»åè®®ç‰¹å®šå­—æ®µæå–
            if not name_to_check:
                # ä½¿ç”¨å­—å…¸æ˜ å°„åè®®ç±»å‹åˆ°å¯¹åº”çš„åç§°æå–å‡½æ•°ï¼Œæé«˜å¯ç»´æŠ¤æ€§
                protocol_handlers = {
                    # Vmessåè®®
                    'vmess://': get_vmess_name,
                    # Vlessåè®®
                    'vless://': get_vless_name,
                    # Trojanåè®®
                    'trojan://': get_trojan_name,
                    # ShadowSocksåè®®
                    'ss://': get_shadowsocks_name,
                    # ShadowSocksRåè®®
                    'ssr://': get_ssr_name,
                    # WireGuardåè®®
                    'wireguard://': get_wireguard_name,
                    'wg://': get_wireguard_name,
                    # Tuicåè®®
                    'tuic://': get_tuic_name,
                    # Hysteria2åè®®
                    'hy2://': get_hysteria2_name,
                    'hysteria2://': get_hysteria2_name
                }
                
                for prefix, handler_func in protocol_handlers.items():
                    if config.startswith(prefix):
                        name_to_check = handler_func(config)
                        break
                # å…¶ä»–åè®®çš„åç§°æå–æ”¯æŒ

            # å¦‚æœæ— æ³•è·å–åç§°ï¼Œè®°å½•å¹¶è·³è¿‡æ­¤é…ç½®
            if not name_to_check or not isinstance(name_to_check, str):
                logging.debug(f"æ— æ³•ä»é…ç½®ä¸­æå–æœ‰æ•ˆåç§°ï¼Œè·³è¿‡: {config[:100]}...")
                continue
                
            current_name_to_check_str = name_to_check.strip()

            # éå†æ¯ä¸ªå›½å®¶çš„å…³é”®è¯åˆ—è¡¨ï¼Œå¯»æ‰¾åŒ¹é…
            country_matched = False
            for country_name_key, keywords_for_country_list in country_keywords_for_naming.items():
                # åªå¤„ç†æœ‰æ•ˆçš„å…³é”®è¯åˆ—è¡¨
                if not isinstance(keywords_for_country_list, list):
                    continue
                    
                # å‡†å¤‡æ­¤å›½å®¶çš„æ–‡æœ¬å…³é”®è¯ï¼Œè¿‡æ»¤æ— æ•ˆå’Œé‡å¤çš„å…³é”®è¯
                text_keywords_for_country = []
                for kw in keywords_for_country_list:
                    if isinstance(kw, str) and kw.strip():
                        if kw not in text_keywords_for_country:  # é¿å…é‡å¤å…³é”®è¯
                            text_keywords_for_country.append(kw)
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å…³é”®è¯
                match_found = False
                current_name_lower = current_name_to_check_str.lower()
                
                for keyword in text_keywords_for_country:
                    if not isinstance(keyword, str) or not keyword.strip():
                        continue
                        
                    keyword = keyword.strip()
                    keyword_lower = keyword.lower()
                    
                    # ç®€å•æœ‰æ•ˆçš„åŒ¹é…ç­–ç•¥
                    # 1. å¯¹äºç¼©å†™ä½¿ç”¨ç‰¹æ®Šå¤„ç†
                    if len(keyword) in [2, 3] and keyword.isupper() and keyword.isalpha():
                        # æ£€æŸ¥æ˜¯å¦ä½œä¸ºç‹¬ç«‹éƒ¨åˆ†å‡ºç°
                        if keyword_lower in current_name_lower:
                            parts = re.split(r'[^a-zA-Z]', current_name_lower)
                            if keyword_lower in parts:
                                match_found = True
                                break
                    # 2. å¯¹äºæ™®é€šå…³é”®è¯ä½¿ç”¨ç®€å•çš„åŒ…å«åŒ¹é…
                    elif (keyword_lower in current_name_lower or 
                          keyword in current_name_to_check_str):
                        match_found = True
                        break
                
                if match_found:
                    final_configs_by_country[country_name_key].add(config)
                    country_matched = True
                    logging.debug(f"é…ç½®å·²å…³è”åˆ°å›½å®¶: {country_name_key}")
                    # ç§»é™¤è¿™é‡Œçš„breakï¼Œå…è®¸é…ç½®åŒ¹é…å¤šä¸ªå›½å®¶
                
            # ç§»é™¤è¿™é‡Œçš„breakï¼Œç¡®ä¿æ¯ä¸ªé…ç½®éƒ½èƒ½è¢«å®Œå…¨å¤„ç†

    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯æ—¥å¿—ï¼ŒåŒæ—¶å†™å…¥æ—¥å¿—æ–‡ä»¶å’Œæ§åˆ¶å°
    logging.info(f"å¤„ç†ç»Ÿè®¡:")
    logging.info(f"  - æˆåŠŸå¤„ç†é¡µé¢: {processed_pages}/{len(fetched_pages)}")
    logging.info(f"  - æ‰¾åˆ°æœ‰æ•ˆé…ç½®: {found_configs}")
    logging.info(f"  - è¿‡æ»¤æ— æ•ˆé…ç½®: {filtered_out_configs}")
    logging.info(f"  - è¿‡æ»¤ç‡: {filtered_out_configs/(found_configs+filtered_out_configs)*100:.1f}%" if (found_configs+filtered_out_configs) > 0 else "  - æ— é…ç½®æ‰¾åˆ°")
    

    # å‡†å¤‡è¾“å‡ºç›®å½•ç»“æ„
    country_dir = os.path.join(OUTPUT_DIR, COUNTRY_SUBDIR)
    protocol_dir = os.path.join(OUTPUT_DIR, PROTOCOL_SUBDIR)
    
    # ä½¿ç”¨ç»å¯¹è·¯å¾„
    abs_output_dir = os.path.abspath(OUTPUT_DIR)
    abs_country_dir = os.path.abspath(country_dir)
    abs_protocol_dir = os.path.abspath(protocol_dir)
    
    # å¢å¼ºçš„ç›®å½•å¤„ç†é€»è¾‘
    try:
        # 1. å®‰å…¨åœ°åˆ é™¤æ—§ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        def safe_rmtree(path):
            if os.path.exists(path):
                try:
                    shutil.rmtree(path)
                    return True
                except Exception as e:
                    logging.warning(f"æ— æ³•åˆ é™¤ç›®å½• {path}: {e}")
                    return False
            return True
        
        # å…ˆåˆ é™¤å­ç›®å½•ï¼Œå†åˆ é™¤çˆ¶ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        safe_rmtree(abs_country_dir)
        safe_rmtree(abs_protocol_dir)
        
        # 2. ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨ä¸”å¯å†™
        parent_dir = os.path.dirname(abs_output_dir)
        if parent_dir and not os.path.exists(parent_dir):
            try:
                os.makedirs(parent_dir, exist_ok=True)
                logging.debug(f"åˆ›å»ºçˆ¶ç›®å½•: {parent_dir}")
            except Exception as e:
                logging.error(f"æ— æ³•åˆ›å»ºçˆ¶ç›®å½•: {parent_dir}, é”™è¯¯: {e}")
                return
        
        # 3. æ£€æŸ¥å†™å…¥æƒé™
        if not os.access(parent_dir or os.getcwd(), os.W_OK):
            logging.error(f"é”™è¯¯: æ²¡æœ‰å†™å…¥æƒé™: {parent_dir or os.getcwd()}")
            return
        
        # 4. åˆ›å»ºæ–°çš„ç›®å½•ç»“æ„
        for dir_path in [abs_output_dir, abs_country_dir, abs_protocol_dir]:
            try:
                os.makedirs(dir_path, exist_ok=True)
                logging.debug(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {dir_path}")
            except Exception as e:
                logging.error(f"åˆ›å»ºç›®å½•å¤±è´¥: {dir_path}, é”™è¯¯: {e}")
                return
        
        # 5. æœ€ç»ˆæƒé™éªŒè¯
        for dir_path in [abs_output_dir, abs_country_dir, abs_protocol_dir]:
            if not os.path.isdir(dir_path) or not os.access(dir_path, os.W_OK):
                logging.error(f"ç›®å½•æ£€æŸ¥å¤±è´¥: {dir_path}")
                return
        
        logging.info(f"ç›®å½•ç»“æ„å‡†å¤‡å®Œæˆ:")
        logging.info(f"  - è¾“å‡ºç›®å½•: {abs_output_dir}")
        logging.info(f"  - å›½å®¶ç›®å½•: {abs_country_dir}")
        logging.info(f"  - åè®®ç›®å½•: {abs_protocol_dir}")
        
    except Exception as e:
        logging.error(f"ç›®å½•å¤„ç†å¤±è´¥: {e}")
        return
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶éªŒè¯å†™å…¥æƒé™
        test_file = os.path.join(abs_output_dir, "test_write.txt")
        try:
            with open(test_file, 'w') as f:
                f.write("æµ‹è¯•å†™å…¥æƒé™")
            if os.path.exists(test_file):
                logging.info(f"âœ“ å†™å…¥æµ‹è¯•æˆåŠŸ")
                os.remove(test_file)  # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            else:
                logging.error(f"âœ— å†™å…¥æµ‹è¯•å¤±è´¥: æ–‡ä»¶æœªåˆ›å»º")
        except Exception as write_test_e:
            logging.error(f"âœ— å†™å…¥æµ‹è¯•å¤±è´¥: {str(write_test_e)}")

    # ä¿å­˜åè®®é…ç½®æ–‡ä»¶
    protocol_counts = {}
    protocol_category_count = 0
    
    logging.info(f"å¼€å§‹ä¿å­˜åè®®é…ç½®æ–‡ä»¶åˆ°: {abs_protocol_dir}")
    
    # é¢„å…ˆè¿‡æ»¤å‡ºéç©ºåè®®ç±»åˆ«
    non_empty_protocols = {cat: items for cat, items in final_all_protocols.items() if items}
    
    # æŒ‰åè®®ç±»å‹æ’åºï¼Œæé«˜å¯é¢„æµ‹æ€§
    for category, items in sorted(non_empty_protocols.items()):
        items_count = len(items)
        logging.info(f"ä¿å­˜åè®® {category}: {items_count} ä¸ªé…ç½®")
        
        saved, count = save_to_file(protocol_dir, category, items)
        if saved:
            protocol_counts[category] = count
            protocol_category_count += 1
        else:
            logging.error(f"ä¿å­˜åè®® {category} å¤±è´¥")
        
        # å†…å­˜ä¼˜åŒ–ï¼šä¿å­˜åæ¸…ç†å¤§å‹é›†åˆ
        final_all_protocols[category].clear()
    
    total_protocol_configs = sum(protocol_counts.values())
    logging.info(f"åè®®é…ç½®ä¿å­˜å®Œæˆ: æˆåŠŸ {protocol_category_count}/{len(non_empty_protocols)} ä¸ªç±»åˆ«, æ€»è®¡ {total_protocol_configs} é¡¹")
    
    # ä¿å­˜å›½å®¶é…ç½®æ–‡ä»¶
    country_counts = {}
    countries_with_configs = 0
    total_country_configs = 0
    
    logging.info(f"å¼€å§‹ä¿å­˜å›½å®¶é…ç½®æ–‡ä»¶åˆ°: {abs_country_dir}")
    
    # é¢„å…ˆè¿‡æ»¤å‡ºéç©ºå›½å®¶ç±»åˆ«
    non_empty_countries = {cat: items for cat, items in final_configs_by_country.items() if items}
    
    for category, items in non_empty_countries.items():
        actual_count = len(items)
        logging.debug(f"ä¿å­˜å›½å®¶ {category} çš„ {actual_count} ä¸ªé…ç½®")
        
        saved, count = save_to_file(country_dir, category, items)
        if saved:
            country_counts[category] = actual_count
            countries_with_configs += 1
            total_country_configs += actual_count
    
    logging.info(f"å›½å®¶é…ç½®ä¿å­˜å®Œæˆ: æˆåŠŸ {countries_with_configs}/{len(non_empty_countries)} ä¸ªå›½å®¶, æ€»è®¡ {total_country_configs} é¡¹")
    
    # ç”ŸæˆREADMEæ–‡ä»¶
    try:
        generate_simple_readme(protocol_counts, country_counts, categories_data, use_local_paths=True)
    except Exception as e:
        logging.error(f"ç”ŸæˆREADMEæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        # ç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­ç¨‹åº
    
    # è¾“å‡ºå®Œæˆä¿¡æ¯
    logging.info(f"=== æŠ“å–å®Œæˆ ===")
    logging.info(f"æ‰¾åˆ°å¹¶ä¿å­˜çš„åè®®é…ç½®: {total_protocol_configs}")
    logging.info(f"æœ‰é…ç½®çš„å›½å®¶æ•°é‡: {countries_with_configs}")
    logging.info(f"å›½å®¶ç›¸å…³é…ç½®æ€»æ•°: {total_country_configs}")
    logging.info(f"è¾“å‡ºç›®å½•: {abs_output_dir}")
    logging.info(f"å›½å®¶é…ç½®ç›®å½•: {abs_country_dir}")
    logging.info(f"åè®®é…ç½®ç›®å½•: {abs_protocol_dir}")
    logging.info(f"READMEæ–‡ä»¶å·²æ›´æ–°")

async def cleanup_tasks():
    """å®‰å…¨æ¸…ç†æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„å¼‚æ­¥ä»»åŠ¡ï¼Œå¸¦è¶…æ—¶å’Œé”™è¯¯å¤„ç†"""
    try:
        # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼Œå¤„ç†å¯èƒ½çš„RuntimeError
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            # å¦‚æœæ²¡æœ‰æ­£åœ¨è¿è¡Œçš„å¾ªç¯ï¼Œä½¿ç”¨å½“å‰å¾ªç¯æˆ–åˆ›å»ºæ–°å¾ªç¯
            loop = asyncio.get_event_loop()
        
        # è·å–æ‰€æœ‰ä»»åŠ¡å¹¶è¿‡æ»¤å‡ºéœ€è¦å–æ¶ˆçš„ä»»åŠ¡
        tasks = asyncio.all_tasks(loop=loop)
        current_task = asyncio.current_task(loop=loop)
        
        # åªå–æ¶ˆæœªå®Œæˆä¸”ä¸æ˜¯å½“å‰ä»»åŠ¡çš„ä»»åŠ¡
        tasks_to_cancel = [task for task in tasks if task != current_task and not task.done()]
        
        if not tasks_to_cancel:
            logging.debug("æ²¡æœ‰éœ€è¦æ¸…ç†çš„å¼‚æ­¥ä»»åŠ¡")
            return
        
        logging.info(f"å¼€å§‹æ¸…ç† {len(tasks_to_cancel)} ä¸ªæ­£åœ¨è¿è¡Œçš„å¼‚æ­¥ä»»åŠ¡")
        
        # å–æ¶ˆæ‰€æœ‰éœ€è¦æ¸…ç†çš„ä»»åŠ¡
        for task in tasks_to_cancel:
            task.cancel()
        
        # ä½¿ç”¨å¸¦è¶…æ—¶çš„gatherï¼Œé¿å…æ— é™ç­‰å¾…
        try:
            # è®¾ç½®10ç§’è¶…æ—¶ï¼Œé˜²æ­¢ä»»åŠ¡æ¸…ç†è¿‡ç¨‹å¡ä½
            results = await asyncio.wait_for(
                asyncio.gather(*tasks_to_cancel, return_exceptions=True),
                timeout=10.0
            )
            
            # ç»Ÿè®¡æ¸…ç†ç»“æœ
            cancelled_count = sum(1 for r in results if isinstance(r, asyncio.CancelledError))
            error_count = sum(1 for r in results if isinstance(r, Exception) and not isinstance(r, asyncio.CancelledError))
            
            logging.info(f"ä»»åŠ¡æ¸…ç†å®Œæˆ: å·²å–æ¶ˆ {cancelled_count}, é”™è¯¯ {error_count}")
            
            # è®°å½•éå–æ¶ˆçš„å¼‚å¸¸ï¼ˆå¦‚æœæœ‰ï¼‰
            for i, result in enumerate(results):
                if isinstance(result, Exception) and not isinstance(result, asyncio.CancelledError):
                    logging.debug(f"ä»»åŠ¡æ¸…ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {result}")
                    
        except asyncio.TimeoutError:
            logging.warning("ä»»åŠ¡æ¸…ç†è¶…æ—¶ï¼Œå¯èƒ½æœ‰ä»»åŠ¡ä»åœ¨è¿è¡Œ")
        except Exception as e:
            logging.error(f"ä»»åŠ¡æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            
    except Exception as e:
        logging.error(f"æ¸…ç†ä»»åŠ¡å‡½æ•°æœ¬èº«å‘ç”Ÿå¼‚å¸¸: {e}")

def safe_cleanup():
    """å®‰å…¨çš„èµ„æºæ¸…ç†å‡½æ•°ï¼Œç¡®ä¿æ‰€æœ‰èµ„æºè¢«æ­£ç¡®é‡Šæ”¾"""
    try:
        logging.info("=== å¼€å§‹èµ„æºæ¸…ç† ===")
        
        # 1. æ¸…ç†å¼‚æ­¥ä»»åŠ¡
        loop = None
        try:
            # å°è¯•è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                # æ²¡æœ‰ç°æœ‰å¾ªç¯ï¼Œåˆ›å»ºæ–°å¾ªç¯
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
            # å®‰å…¨è¿è¡Œæ¸…ç†ä»»åŠ¡
            if loop.is_running():
                # åœ¨è¿è¡Œä¸­çš„å¾ªç¯ä½¿ç”¨call_soon_threadsafe
                logging.debug("åœ¨è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ä¸Šè°ƒåº¦æ¸…ç†ä»»åŠ¡")
                loop.call_soon_threadsafe(lambda: asyncio.create_task(cleanup_tasks()))
                
                # ç»™ä»»åŠ¡ä¸€äº›æ—¶é—´æ‰§è¡Œï¼ˆéé˜»å¡ï¼‰
                import time
                time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿ
            else:
                # åœ¨æœªè¿è¡Œçš„å¾ªç¯ä¸Šç›´æ¥æ‰§è¡Œ
                try:
                    logging.debug("åœ¨æ–°äº‹ä»¶å¾ªç¯ä¸Šè¿è¡Œæ¸…ç†ä»»åŠ¡")
                    # ä½¿ç”¨run_until_completeè¿è¡Œæ¸…ç†ä»»åŠ¡
                    loop.run_until_complete(cleanup_tasks())
                    
                    # ç¡®ä¿å¼‚æ­¥ç”Ÿæˆå™¨è¢«å…³é—­
                    try:
                        loop.run_until_complete(loop.shutdown_asyncgens())
                    except AttributeError:
                        # Python 3.6åŠæ›´æ—©ç‰ˆæœ¬æ²¡æœ‰shutdown_asyncgens
                        pass
                        
                except Exception as e:
                    logging.error(f"è¿è¡Œæ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
                    
                    logging.debug(f"æ¸…ç†ä»»åŠ¡é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                    
        except Exception as e:
            logging.error(f"äº‹ä»¶å¾ªç¯æ“ä½œæ—¶å‡ºé”™: {e}")
        finally:
            # ç¡®ä¿å…³é—­å¾ªç¯
            try:
                if loop and not loop.is_closed():
                    loop.close()
                    logging.debug("äº‹ä»¶å¾ªç¯å·²å…³é—­")
            except Exception as e:
                logging.error(f"å…³é—­äº‹ä»¶å¾ªç¯æ—¶å‡ºé”™: {e}")
        
        # 2. æ¸…ç†å…¶ä»–èµ„æºï¼ˆå¦‚æœæœ‰ï¼‰
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–èµ„æºæ¸…ç†é€»è¾‘
        
        logging.info("=== èµ„æºæ¸…ç†å®Œæˆ ===")
        
    except Exception as e:
        logging.error(f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸: {e}")

if __name__ == "__main__":
    start_time = time.time()
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        import sys
        import warnings
        
        # æ•è·å¹¶è®°å½•è­¦å‘Š
        warnings.filterwarnings('always')
        
        # æ‰“å°å¯åŠ¨ä¿¡æ¯
        logging.info("=== V2Rayé…ç½®æŠ“å–å·¥å…·å¯åŠ¨ ===")
        logging.info(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        logging.info(f"Pythonç‰ˆæœ¬: {sys.version}")
        logging.info(f"è„šæœ¬è·¯å¾„: {os.path.abspath(__file__)}")
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬å…¼å®¹æ€§
        if sys.version_info < (3, 7):
            logging.warning("è­¦å‘Š: æ¨èä½¿ç”¨Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
            
        # è¿è¡Œä¸»ç¨‹åº
        asyncio.run(main())
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = time.time() - start_time
        logging.info(f"ç¨‹åºæˆåŠŸå®Œæˆï¼æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
        
    except KeyboardInterrupt:
        logging.info("âš ï¸  ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except asyncio.TimeoutError:
        logging.error("â±ï¸  ç¨‹åºæ‰§è¡Œè¶…æ—¶")
        
        logging.debug(f"è¶…æ—¶é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    except aiohttp.ClientError as e:
        logging.error(f"ğŸŒ HTTPå®¢æˆ·ç«¯é”™è¯¯: {str(e)}")
        
        logging.debug(f"HTTPé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    except ValueError as e:
        logging.error(f"ğŸ“Š æ•°æ®å¤„ç†é”™è¯¯: {str(e)}")
        
        logging.debug(f"æ•°æ®é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    except FileNotFoundError as e:
        logging.error(f"ğŸ“ æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
        
        logging.debug(f"æ–‡ä»¶é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    except IOError as e:
        logging.error(f"ğŸ’¾ IOé”™è¯¯: {str(e)}")
        
        logging.debug(f"IOé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    except PermissionError as e:
        logging.error(f"ğŸ”’ æƒé™é”™è¯¯: {str(e)}")
        
        logging.debug(f"æƒé™é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    except OSError as e:
        logging.error(f"ğŸ–¥ï¸  æ“ä½œç³»ç»Ÿé”™è¯¯: {str(e)}")
        
        logging.debug(f"OSé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
    except Exception as e:
        logging.critical(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        logging.debug(f"é”™è¯¯è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}")
        
        # æä¾›ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        print("\nç¨‹åºå‘ç”Ÿé”™è¯¯ã€‚è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ³•:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å’Œæ–‡ä»¶æƒé™")
        print("3. æ›´æ–°ä¾èµ–åº“: pip install -r requirements.txt")
        print("4. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯")
    finally:
        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        total_time = time.time() - start_time
        logging.info(f"=== ç¨‹åºç»“æŸ === æ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
        
        # æ‰§è¡Œå®‰å…¨æ¸…ç†
        safe_cleanup()
